# Eliciting latent knowledge from language reward models

Code for my thesis titled "Eliciting latent knowledge from language reward models" for the MPhil in Machine Learning and Machine Intelligence at the University of Cambridge.


## Idea

Use methods that _discover latent knowledge_ (DLK), such as [CCS](https://arxiv.org/abs/2212.03827), to build reward models that promote truthfulness. Utilize these reward models to execute _reinforcement learning_ (RL) fine-tuning to improve the "truthfulness" of LLMs.

For more details, see the accompanying [blog post](link).


## Installation and prerequisites

1. Clone the repository.
1. Install the `EleutherAI/elk` library. I personally used the version from [this](https://github.com/EleutherAI/elk/tree/a2904e62765fa311b1197505f78fab295e1c87fb) commit. Installation instructions can be found in the README of the provided link.
1. Install the _Language Model Evaluation Harness_ ([EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)). To make sure my results match with _Open LLM Leaderboard_ ([link](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)), I used [this](https://github.com/EleutherAI/lm-evaluation-harness/tree/b281b0921b636bc36ad05c0b0b0763bd6dd43463) version of the harness.


## Usage

To be supplemented.

