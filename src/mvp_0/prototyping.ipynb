{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fsx/home-augustas/elk\n",
      "['/fsx/home-augustas/elk/elk/promptsource', '/fsx/home-augustas/elk/elk/training', '/fsx/home-augustas/elk']\n"
     ]
    }
   ],
   "source": [
    "ELK_PATH = Path(\"../../../elk/\")\n",
    "print(ELK_PATH.resolve())\n",
    "\n",
    "modules = [\n",
    "    ELK_PATH,\n",
    "    ELK_PATH / \"elk\" / \"training\",\n",
    "    ELK_PATH / \"elk\" / \"promptsource\",\n",
    "]\n",
    "\n",
    "for module in modules:\n",
    "    if not str(module) in sys.path:\n",
    "        sys.path.insert(0, str(module.resolve()))\n",
    "\n",
    "print(sys.path[:3])\n",
    "\n",
    "from reporter import Reporter\n",
    "from templates import DatasetTemplates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg.yaml  fingerprints.yaml  lr_eval.csv  reporters\t  train_lm_eval.csv\n",
      "eval.csv  lm_eval.csv\t     lr_models\t  train_eval.csv\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = (\n",
    "    \"../../../VINC-logs/\"\n",
    "    \"allenai/unifiedqa-v2-t5-11b-1363200/\"\n",
    "    \"AugustasM/burns-datasets-VINC/strange-montalcini\"\n",
    ")\n",
    "DATA_DIR = Path(DATA_DIR)\n",
    "\n",
    "!ls {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"allenai/unifiedqa-v2-t5-11b-1363200\"\n",
    "model_name = \"allenai/unifiedqa-v2-t5-3b-1363200\"\n",
    "\n",
    "LAYER = 24"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/admin/home-augustas/.cache/huggingface/datasets/AugustasM___parquet/AugustasM--burns-datasets-VINC-85ec467026b56702/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'original_dataset', 'template_name'],\n",
       "    num_rows: 64\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"AugustasM/burns-datasets-VINC\", split=\"validation[:64]\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Consider the text below': <templates.Template object at 0x7f213f2c5de0>, 'Is the given text truthful': <templates.Template object at 0x7f213f2c5e10>, 'Text first': <templates.Template object at 0x7f213f2c5e40>, 'No question no choices': <templates.Template object at 0x7f213f2c4c70>, 'No question with choices': <templates.Template object at 0x7f213f2c5d80>}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<templates.Template at 0x7f213f2c5de0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_template_path = \"AugustasM/burns-datasets-VINC/all\"\n",
    "\n",
    "dataset_templates = DatasetTemplates(dataset_template_path)\n",
    "dataset_templates.templates = {\n",
    "    x.name: x for x in dataset_templates.templates.values() if x.get_answer_choices_list(dataset[0]) is not None\n",
    "}\n",
    "print(dataset_templates.templates)\n",
    "\n",
    "template = list(dataset_templates.templates.values())[0]\n",
    "template"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'allenai/unifiedqa-v2-t5-3b-1363200'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Tokenizer(name_or_path='allenai/unifiedqa-v2-t5-3b-1363200', vocab_size=32100, model_max_length=512, is_fast=False, padding_side='right', truncation_side='left', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name, truncation_side=\"left\")\n",
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try combining report with a language model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspiration taken from the [original repository](https://github.com/collin-burns/discovering_latent_knowledge/blob/main/CCS.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyRewardModel(nn.Module):\n",
    "#     def __init__(\n",
    "#             self, language_model_name, reporter_path,\n",
    "#             layer=-1, device=\"cpu\", hidden_state_name=\"decoder_hidden_states\",\n",
    "#         ):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Load the language model and the reporter\n",
    "#         self.language_model = T5ForConditionalGeneration.from_pretrained(\n",
    "#             language_model_name\n",
    "#         ).to(device)\n",
    "#         self.language_model.eval()\n",
    "\n",
    "#         self.reporter = Reporter.load(reporter_path).to(device)\n",
    "#         self.reporter.eval()\n",
    "\n",
    "#         self.layer = layer # which layer to extract\n",
    "#         self.hidden_state_name = hidden_state_name\n",
    "\n",
    "    \n",
    "#     def forward(self, pos_inputs, neg_inputs):\n",
    "#         # Get the hidden states\n",
    "#         pos_hidden_states = self.language_model(\n",
    "#             **pos_inputs, output_hidden_states=True,\n",
    "#         )[self.hidden_state_name][self.layer]\n",
    "#         neg_hidden_states = self.language_model(\n",
    "#             **neg_inputs, output_hidden_states=True,\n",
    "#         )[self.hidden_state_name][self.layer]\n",
    "        \n",
    "#         # Find the index of the last non-padding token\n",
    "#         pos_last_token_index = torch.sum(pos_inputs[\"attention_mask\"], dim=1) - 1\n",
    "#         neg_last_token_index = torch.sum(neg_inputs[\"attention_mask\"], dim=1) - 1\n",
    "\n",
    "#         # Get the last token's output\n",
    "#         pos_last_tokens = pos_hidden_states[range(len(pos_last_token_index)), pos_last_token_index]\n",
    "#         neg_last_tokens = neg_hidden_states[range(len(neg_last_token_index)), neg_last_token_index]\n",
    "\n",
    "#         # Get the logits for the two classes\n",
    "#         pos_logits = self.reporter(pos_last_tokens)\n",
    "#         neg_logits = self.reporter(neg_last_tokens)\n",
    "\n",
    "#         # Return the difference in logits which will later be\n",
    "#         # passed through a sigmoid function\n",
    "#         return pos_logits - neg_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRewardModel(nn.Module):\n",
    "    def __init__(\n",
    "            self, language_model, reporter_path,\n",
    "            layer=-1, device=\"cpu\", hidden_state_name=\"decoder_hidden_states\",\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load the language model and the reporter\n",
    "        # self.language_model = T5ForConditionalGeneration.from_pretrained(\n",
    "        #     language_model\n",
    "        # ).to(device)\n",
    "        # self.language_model.eval()\n",
    "        self.language_model = language_model\n",
    "\n",
    "        self.reporter = Reporter.load(reporter_path).to(device)\n",
    "        self.reporter.eval()\n",
    "\n",
    "        self.layer = layer # which layer to extract\n",
    "        self.hidden_state_name = hidden_state_name\n",
    "\n",
    "    \n",
    "    def forward(self, pos_inputs, neg_inputs):\n",
    "        '''\n",
    "            NOTE: only works for a single input at a time for now\n",
    "        '''\n",
    "\n",
    "        # Get the hidden states\n",
    "        pos_hidden_states = self.language_model(\n",
    "            **pos_inputs, output_hidden_states=True,\n",
    "        )[self.hidden_state_name][self.layer]\n",
    "        neg_hidden_states = self.language_model(\n",
    "            **neg_inputs, output_hidden_states=True,\n",
    "        )[self.hidden_state_name][self.layer]\n",
    "\n",
    "        # Get the last token's output\n",
    "        # Shape B x T x H -> B x H\n",
    "        pos_last_tokens = pos_hidden_states[:, -1, :]\n",
    "        neg_last_tokens = neg_hidden_states[:, -1, :]\n",
    "\n",
    "        # Get the logits for the two classes\n",
    "        # Shape B x H -> B\n",
    "        pos_logits = self.reporter(pos_last_tokens)\n",
    "        neg_logits = self.reporter(neg_last_tokens)\n",
    "\n",
    "        # Return the difference in logits which will later be\n",
    "        # passed through a sigmoid function\n",
    "        return pos_logits - neg_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/fsx/home-augustas/VINC-logs/allenai/unifiedqa-v2-t5-11b-1363200/AugustasM/burns-datasets-VINC/strange-montalcini/reporters/layer_24.pt')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reporter_path = DATA_DIR / \"reporters\" / f\"layer_{LAYER}.pt\"\n",
    "reporter_path.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 79.3 ms, sys: 0 ns, total: 79.3 ms\n",
      "Wall time: 8.24 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "reward_model = MyRewardModel(model, reporter_path, layer=LAYER, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = dataset[0]\n",
    "item_copy = item.copy()\n",
    "\n",
    "# Get the positive and negative examples\n",
    "item_copy[\"label\"] = 1\n",
    "pos_q, pos_a = template.apply(item_copy)\n",
    "\n",
    "item_copy[\"label\"] = 0\n",
    "neg_q, neg_a = template.apply(item_copy)\n",
    "\n",
    "# Tokenize the inputs\n",
    "pos_inputs = tokenization_function(pos_q, pos_a).to(device)\n",
    "neg_inputs = tokenization_function(neg_q, neg_a).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2552], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = reward_model(pos_inputs, neg_inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sift through the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_function = lambda q, a: tokenizer(\n",
    "    q, text_target=a.strip(),\n",
    "    add_special_tokens=True, return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.88 s, sys: 28.1 ms, total: 5.9 s\n",
      "Wall time: 6.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.2552, 0.3842, 0.3257, 0.2843, 0.3764]), tensor([0, 0, 0, 1, 1]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "for idx, item in enumerate(dataset):\n",
    "    labels.append(item[\"label\"])\n",
    "    item_copy = item.copy()\n",
    "\n",
    "    # Get the positive and negative examples\n",
    "    item_copy[\"label\"] = 1\n",
    "    pos_q, pos_a = template.apply(item_copy)\n",
    "\n",
    "    item_copy[\"label\"] = 0\n",
    "    neg_q, neg_a = template.apply(item_copy)\n",
    "\n",
    "    # Tokenize the inputs\n",
    "    pos_inputs = tokenization_function(pos_q, pos_a).to(device)\n",
    "    neg_inputs = tokenization_function(neg_q, neg_a).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = reward_model(pos_inputs, neg_inputs)\n",
    "    \n",
    "    predictions.append(prediction.item())\n",
    "\n",
    "predictions = torch.tensor(predictions)\n",
    "labels = torch.tensor(labels)\n",
    "predictions[:5], labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
