{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tokenizer(model_str: str, **kwargs):\n",
    "    \"\"\"Instantiate a tokenizer, using the fast one iff it exists.\"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_str, use_fast=True, **kwargs)\n",
    "    \n",
    "    except Exception as e:\n",
    "        if kwargs.get(\"verbose\", True):\n",
    "            print(f\"Falling back to slow tokenizer; fast one failed: '{e}'\")\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_str, use_fast=False, **kwargs)\n",
    "\n",
    "    if getattr(tokenizer, \"pad_token\", None) is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"EleutherAI/gpt-j-6B\")\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/admin/home-augustas/.cache/huggingface/datasets/AugustasM___parquet/AugustasM--burns-ppo-training-dataset-acfd6af5c398b8b6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "Loading cached processed dataset at /admin/home-augustas/.cache/huggingface/datasets/AugustasM___parquet/AugustasM--burns-ppo-training-dataset-acfd6af5c398b8b6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-5dcada4795f777c4.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['original_dataset', 'template_name', 'prompt', 'best_response'],\n",
       "    num_rows: 8718\n",
       "})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"AugustasM/burns-ppo-training-dataset\", split=\"train\")\n",
    "dataset = dataset.filter(lambda x: x[\"original_dataset\"] != \"piqa\")\n",
    "# dataset = dataset.filter(lambda x: x[\"original_dataset\"] != \"super_glue/copa\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not need to truncate for GPT-J 6B, check for other models\n",
    "def tokenize(batch, max_length=1024):\n",
    "    return tokenizer(\n",
    "        batch[\"prompt\"], padding=\"max_length\",\n",
    "        max_length=max_length, return_tensors=\"pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Loading cached processed dataset at /admin/home-augustas/.cache/huggingface/datasets/AugustasM___parquet/AugustasM--burns-ppo-training-dataset-acfd6af5c398b8b6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-0ac5a196cc334eea.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['original_dataset', 'template_name', 'prompt', 'best_response', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 8718\n",
       "})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"EleutherAI/gpt-j-6B\")\n",
    "type(tokenizer)\n",
    "\n",
    "prompt_max_len = max(\n",
    "    tokenizer(row[\"prompt\"], return_tensors=\"pt\")[\"input_ids\"].shape[1] for row in dataset\n",
    ")\n",
    "print(prompt_max_len)\n",
    "\n",
    "processed_dataset = dataset.map(tokenize, batched=True, fn_kwargs={ \"max_length\": prompt_max_len })\n",
    "processed_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "first_len = processed_dataset[0][\"input_ids\"].shape[0]\n",
    "print(first_len)\n",
    "print(all(len(row[\"input_ids\"]) == first_len for row in processed_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(row[\"input_ids\"].shape[0] for row in processed_dataset)) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"EleutherAI/gpt-j-6B\")\n",
    "type(tokenizer)\n",
    "\n",
    "prompt_max_len = max(\n",
    "    tokenizer(row[\"best_response\"], return_tensors=\"pt\")[\"input_ids\"].shape[1] for row in dataset\n",
    ")\n",
    "print(prompt_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"EleutherAI/gpt-j-6B\")\n",
    "type(tokenizer)\n",
    "\n",
    "prompt_max_len = min(\n",
    "    tokenizer(row[\"best_response\"], return_tensors=\"pt\")[\"input_ids\"].shape[1] for row in dataset\n",
    ")\n",
    "print(prompt_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider the following example: ''' 49ers #39; home to be renamed Monster Park after stereo cable company Some fans think 44-year-old Candlestick Park is already a dinosaur. Now the San Francisco 49ers #39; home stadium has the name to match. ''' Choice 1: Sports. Choice 2: World politics.Between choice 1 and choice 2, what is the topic of this example? Sports\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"prompt\"] + dataset[0][\"best_response\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trash bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"EleutherAI/gpt-j-6B\")\n",
    "type(tokenizer)\n",
    "\n",
    "# Find the length and the longest best_response in dataset\n",
    "prompt_max_len = 0\n",
    "longest_prompt = \"\"\n",
    "longest_dataset = \"\"\n",
    "for row in dataset:\n",
    "    tokens = tokenizer(row[\"prompt\"], return_tensors=\"pt\")[\"input_ids\"]\n",
    "    if tokens.shape[1] > max_len:\n",
    "        longest_best_response = row[\"prompt\"]\n",
    "        max_len = tokens.shape[1]\n",
    "        longest_dataset = row[\"original_dataset\"]\n",
    "\n",
    "max_len, longest_dataset, longest_best_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11, 'super_glue/copa', 'I shut off the light in the unoccupied room.')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"EleutherAI/gpt-j-6B\")\n",
    "type(tokenizer)\n",
    "\n",
    "# Find the length and the longest best_response in dataset\n",
    "max_len = 0\n",
    "longest_best_response = \"\"\n",
    "longest_dataset = \"\"\n",
    "for row in dataset:\n",
    "    tokens = tokenizer(row[\"best_response\"], return_tensors=\"pt\")[\"input_ids\"]\n",
    "    if tokens.shape[1] > max_len:\n",
    "        longest_best_response = row[\"best_response\"]\n",
    "        max_len = tokens.shape[1]\n",
    "        longest_dataset = row[\"original_dataset\"]\n",
    "\n",
    "max_len, longest_dataset, longest_best_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
