{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fsx/home-augustas/elk\n",
      "['/fsx/home-augustas/elk/elk/promptsource', '/fsx/home-augustas/elk/elk/training', '/fsx/home-augustas/elk']\n"
     ]
    }
   ],
   "source": [
    "ELK_PATH = Path(\"/fsx/home-augustas/elk/\")\n",
    "print(ELK_PATH.resolve())\n",
    "\n",
    "modules = [\n",
    "    ELK_PATH,\n",
    "    ELK_PATH / \"elk\" / \"training\",\n",
    "    ELK_PATH / \"elk\" / \"promptsource\",\n",
    "]\n",
    "\n",
    "for module in modules:\n",
    "    if not str(module) in sys.path:\n",
    "        sys.path.insert(0, str(module.resolve()))\n",
    "\n",
    "print(sys.path[:3])\n",
    "\n",
    "from reporter import Reporter\n",
    "from templates import DatasetTemplates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '../../../VINC-logs/allenai/unifiedqa-v2-t5-3b-1363200/AugustasM/burns-datasets-VINC/sad-carson': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = (\n",
    "    \"../../../VINC-logs/\"\n",
    "    \"allenai/unifiedqa-v2-t5-3b-1363200/\"\n",
    "    \"AugustasM/burns-datasets-VINC/sad-carson\"\n",
    ")\n",
    "DATA_DIR = Path(DATA_DIR)\n",
    "\n",
    "!ls {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"allenai/unifiedqa-v2-t5-11b-1363200\"\n",
    "# model_name = \"allenai/unifiedqa-v2-t5-large-1363200\"\n",
    "model_name = \"allenai/unifiedqa-v2-t5-3b-1363200\"\n",
    "\n",
    "LAYER = 18\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'allenai/unifiedqa-v2-t5-3b-1363200'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<pad>', '</s>', '<unk>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name, truncation_side=\"left\")\n",
    "print(type(tokenizer))\n",
    "print(tokenizer.is_fast)\n",
    "list(tokenizer.get_vocab().keys())[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/admin/home-augustas/.cache/huggingface/datasets/AugustasM___parquet/AugustasM--burns-datasets-VINC-85ec467026b56702/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'original_dataset', 'template_name'],\n",
       "    num_rows: 64\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"AugustasM/burns-datasets-VINC\", split=f\"validation[:{4*BATCH_SIZE}]\")\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<templates.Template at 0x7f1fbbc87ee0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_template_path = \"AugustasM/burns-datasets-VINC/all\"\n",
    "\n",
    "dataset_templates = DatasetTemplates(dataset_template_path)\n",
    "dataset_templates.templates = {\n",
    "    x.name: x for x in dataset_templates.templates.values() if x.get_answer_choices_list(dataset[0]) is not None\n",
    "}\n",
    "print(len(dataset_templates.templates))\n",
    "\n",
    "template = list(dataset_templates.templates.values())[0]\n",
    "template"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051cca7c560f42848d30ef17099b9bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/64 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f5fe39afa3448c9a70299f2d9913ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/64 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'pos_input_ids', 'pos_attention_mask', 'pos_labels', 'neg_input_ids', 'neg_attention_mask', 'neg_labels'],\n",
       "    num_rows: 64\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenization_function(q, a):\n",
    "    return tokenizer(\n",
    "        q, text_target=a.strip(),\n",
    "        add_special_tokens=True, return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "def preprocess_function(rows):\n",
    "    processed_rows = defaultdict(list)\n",
    "\n",
    "    print(len(rows))\n",
    "    for text in rows[\"text\"]:\n",
    "        entry = { \"text\": text }\n",
    "\n",
    "        # Get the positive and negative examples\n",
    "        entry[\"label\"] = 1\n",
    "        pos_q, pos_a = template.apply(entry)\n",
    "\n",
    "        entry[\"label\"] = 0\n",
    "        neg_q, neg_a = template.apply(entry)\n",
    "\n",
    "        # Tokenize the inputs\n",
    "        pos_inputs = tokenization_function(pos_q, pos_a)\n",
    "        neg_inputs = tokenization_function(neg_q, neg_a)\n",
    "\n",
    "        # Store the processed inputs\n",
    "        processed_rows[\"pos_input_ids\"].append(pos_inputs[\"input_ids\"].squeeze())\n",
    "        processed_rows[\"pos_attention_mask\"].append(pos_inputs[\"attention_mask\"].squeeze())\n",
    "        processed_rows[\"pos_labels\"].append(pos_inputs[\"labels\"].squeeze())\n",
    "        processed_rows[\"neg_input_ids\"].append(neg_inputs[\"input_ids\"].squeeze())\n",
    "        processed_rows[\"neg_attention_mask\"].append(neg_inputs[\"attention_mask\"].squeeze())\n",
    "        processed_rows[\"neg_labels\"].append(neg_inputs[\"labels\"].squeeze())\n",
    "    \n",
    "    return processed_rows\n",
    "        \n",
    "\n",
    "columns_to_delete = dataset.column_names\n",
    "columns_to_delete.remove(\"label\")\n",
    "processed_dataset = dataset.map(\n",
    "    preprocess_function, batched=True, batch_size=BATCH_SIZE,\n",
    "    remove_columns=columns_to_delete,\n",
    ")\n",
    "processed_dataset = processed_dataset.filter(\n",
    "    lambda x: max(len(x[\"pos_input_ids\"]), len(x[\"neg_input_ids\"])) <= tokenizer.model_max_length,\n",
    "    batched=False,\n",
    ")\n",
    "processed_dataset.set_format(type=\"torch\")\n",
    "processed_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try combining report with a language model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspiration taken from the [original repository](https://github.com/collin-burns/discovering_latent_knowledge/blob/main/CCS.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_bf16_possible=True\n",
      "CPU times: user 2min 16s, sys: 4min 48s, total: 7min 4s\n",
      "Wall time: 57.6 s\n"
     ]
    }
   ],
   "source": [
    "model_cfg = AutoConfig.from_pretrained(model_name)\n",
    "fp32_weights = model_cfg.torch_dtype in (None, torch.float32)\n",
    "is_bf16_possible = fp32_weights and torch.cuda.is_bf16_supported()\n",
    "print(f\"{is_bf16_possible=}\")\n",
    "\n",
    "kwargs = {\n",
    "    \"torch_dtype\": torch.bfloat16 if is_bf16_possible else torch.float32\n",
    "}\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, **kwargs).to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2851598336"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRewardModel(nn.Module):\n",
    "    def __init__(\n",
    "            self, language_model, reporter_path,\n",
    "            layer=-1, device=\"cpu\", hidden_state_name=\"decoder_hidden_states\",\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load the language model\n",
    "        if isinstance(language_model, str):\n",
    "            self.language_model = T5ForConditionalGeneration.from_pretrained(\n",
    "                language_model\n",
    "            ).to(device)\n",
    "            self.language_model.eval()\n",
    "        else:\n",
    "            self.language_model = language_model\n",
    "\n",
    "        # Load the reporter\n",
    "        self.reporter = Reporter.load(reporter_path).to(device)\n",
    "        self.reporter.eval()\n",
    "\n",
    "        # Store other variables\n",
    "        self.layer = layer # which layer to extract\n",
    "        self.hidden_state_name = hidden_state_name\n",
    "        self.pad_token_id = self.language_model.config.pad_token_id\n",
    "\n",
    "    \n",
    "    def forward(self, pos_inputs, neg_inputs):\n",
    "        # Get the hidden states\n",
    "        # Shape B x T x H\n",
    "        pos_hidden_states = self.language_model(\n",
    "            **pos_inputs, output_hidden_states=True,\n",
    "        )[self.hidden_state_name][self.layer]\n",
    "        neg_hidden_states = self.language_model(\n",
    "            **neg_inputs, output_hidden_states=True,\n",
    "        )[self.hidden_state_name][self.layer]\n",
    "\n",
    "        pos_last_token_index = (\n",
    "            pos_inputs[\"labels\"] == self.pad_token_id\n",
    "        ).int().argmax(dim=1) - 1\n",
    "        neg_last_token_index = (\n",
    "            neg_inputs[\"labels\"] == self.pad_token_id\n",
    "        ).int().argmax(dim=1) - 1\n",
    "\n",
    "        # Get the last token's output\n",
    "        pos_last_tokens = pos_hidden_states[range(len(pos_last_token_index)), pos_last_token_index]\n",
    "        neg_last_tokens = neg_hidden_states[range(len(neg_last_token_index)), neg_last_token_index]\n",
    "\n",
    "        # Get the logits for the two classes\n",
    "        pos_logits = self.reporter(pos_last_tokens)\n",
    "        neg_logits = self.reporter(neg_last_tokens)\n",
    "\n",
    "        # Return the difference in logits which will later be\n",
    "        # passed through a sigmoid function\n",
    "        return pos_logits - neg_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/fsx/home-augustas/VINC-logs/allenai/unifiedqa-v2-t5-3b-1363200/AugustasM/burns-datasets-VINC/sad-carson/reporters/layer_18.pt')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reporter_path = DATA_DIR / \"reporters\" / f\"layer_{LAYER}.pt\"\n",
    "reporter_path.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model = MyRewardModel(model, reporter_path, layer=LAYER, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sift through the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3945c75cff6440eaa19109c2f06945ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64]) torch.Size([64])\n",
      "CPU times: user 9 s, sys: 8.3 s, total: 17.3 s\n",
      "Wall time: 20 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.4536, 0.4653, 0.5011, 0.4932, 0.4901], device='cuda:0'),\n",
       " tensor([0, 0, 0, 1, 1]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    processed_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=12,\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "loop = tqdm(enumerate(dataloader), total=len(dataloader), leave=False)\n",
    "for idx, batch in loop:\n",
    "    pos_inputs = {\n",
    "        \"input_ids\": batch[\"pos_input_ids\"].to(device),\n",
    "        \"attention_mask\": batch[\"pos_attention_mask\"].to(device),\n",
    "        \"labels\": batch[\"pos_labels\"].to(device),\n",
    "    }\n",
    "    neg_inputs = {\n",
    "        \"input_ids\": batch[\"neg_input_ids\"].to(device),\n",
    "        \"attention_mask\": batch[\"neg_attention_mask\"].to(device),\n",
    "        \"labels\": batch[\"neg_labels\"].to(device),\n",
    "    }\n",
    "    \n",
    "    labels.append(batch[\"label\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        current_predictions = reward_model(pos_inputs, neg_inputs)\n",
    "    \n",
    "    predictions.append(current_predictions.sigmoid())\n",
    "\n",
    "predictions = torch.cat(predictions)\n",
    "labels = torch.cat(labels)\n",
    "print(predictions.shape, labels.shape)\n",
    "predictions[:5], labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
