{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import datasets\n",
    "datasets.logging.set_verbosity_error() # Disable the logging of the datasets library\n",
    "\n",
    "from fastchat.model import get_conversation_template"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = \"v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x  3 augustas Domain Users  25K Jul 21 22:46 qnli_vicuna_ppo_training_raw_v1\n",
      "drwxr-xr-x  3 augustas Domain Users  33K Jul 21 22:45 qnli_vicuna_train_raw_v1\n",
      "-rw-r--r--  1 augustas Domain Users 2.0M Jul 21 23:02 qnli_vicuna_train_v1.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lah datasets | grep {VERSION} | grep qnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    glue/qnli: Dataset({\n",
       "        features: ['question', 'sentence', 'label', 'idx'],\n",
       "        num_rows: 94743\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set dataset_dict to test_dataset\n",
    "dataset_dict = datasets.DatasetDict.load_from_disk(\n",
    "    f\"datasets/qnli_vicuna_ppo_training_raw_{VERSION}\"\n",
    ")\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94743"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dataset.num_rows for dataset in dataset_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glue/qnli: 2\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(f\"{dataset_name}: {len(Counter(dataset['label']))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Consider the sentence below in triple backticks and corresponding question. Does the sentence contain enough information to answer the question? Your answer should be either yes or no.\n",
      "\n",
      "Desired format:\n",
      "Answer: <your_answer>\n",
      "Do not print \"Answer:\" again, just what you think the answer is.\n",
      "\n",
      "Sentence:\n",
      "```\n",
      "The Nippon Hōsō Kyōkai (NHK, the Japan Broadcasting Corporation) began conducting research to \"unlock the fundamental mechanism of video and sound interactions with the five human senses\" in 1964, after the Tokyo Olympics.\n",
      "```\n",
      "Question: What was the aspect ratio of the NHK Color??\n",
      "Answer: ASSISTANT: No\n"
     ]
    }
   ],
   "source": [
    "def format_label(label):\n",
    "    return \"No\" if label else \"Yes\"\n",
    "\n",
    "# Dataset template\n",
    "def template(example, answer_prefix=\"Answer:\"):\n",
    "    conv = get_conversation_template(\"lmsys/vicuna-7b-v1.3\")\n",
    "\n",
    "    message = (\n",
    "        \"Consider the sentence below in triple backticks \"\n",
    "        \"and corresponding question. Does the sentence contain enough information \"\n",
    "        \"to answer the question? Your answer should be either yes or no.\\n\\n\"\n",
    "        \"Desired format:\\n\"\n",
    "        \"Answer: <your_answer>\\n\"\n",
    "        f\"Do not print \\\"{answer_prefix}\\\" again, just what you think the answer is.\\n\\n\"\n",
    "        f\"Sentence:\\n```\\n{example['sentence']}\\n```\\n\"\n",
    "        f\"Question: {example['question']}?\\n\"\n",
    "        f\"{answer_prefix}\"\n",
    "    )\n",
    "\n",
    "    conv.append_message(conv.roles[0], message)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "\n",
    "    return conv.get_prompt(), format_label(example[\"label\"])\n",
    "\n",
    "\n",
    "q, a = template(dataset[2])\n",
    "print(\" \".join([q, a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glue/qnli\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.88 s, sys: 69.2 ms, total: 3.95 s\n",
      "Wall time: 3.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(2023)\n",
    "\n",
    "ALLOWED_KEYS = [\"prompt\", \"best_response\", \"original_dataset\"]\n",
    "\n",
    "new_dataset = []\n",
    "\n",
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(dataset_name)\n",
    "\n",
    "    for idx, entry in enumerate(dataset):\n",
    "        new_entry = entry.copy()\n",
    "        \n",
    "        # In case we need to know which dataset the entry came from\n",
    "        new_entry[\"original_dataset\"] = dataset_name\n",
    "\n",
    "        q, a = template(new_entry)\n",
    "        new_entry[\"prompt\"] = q\n",
    "\n",
    "        # We can now change the label to whether the sample is truthful or not\n",
    "        new_entry[\"best_response\"] = a.strip()\n",
    "\n",
    "        # Remove all other keys\n",
    "        new_entry = { k: v for k, v in new_entry.items() if k in ALLOWED_KEYS }\n",
    "\n",
    "        # Append to the new dataset\n",
    "        new_dataset.append(new_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['original_dataset', 'prompt', 'best_response'],\n",
       "    num_rows: 94743\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset = datasets.Dataset.from_list(new_dataset)\n",
    "my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_dataset': 'glue/qnli',\n",
       " 'prompt': 'A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user\\'s questions. USER: Consider the sentence below in triple backticks and corresponding question. Does the sentence contain enough information to answer the question? Your answer should be either yes or no.\\n\\nDesired format:\\nAnswer: <your_answer>\\nDo not print \"Answer:\" again, just what you think the answer is.\\n\\nSentence:\\n```\\nNondenominational, evangelical, independent and other churches are on the rise, and constitute a significant part of Protestant Christianity.\\n```\\nQuestion: What percentage of Christians are Protestant??\\nAnswer: ASSISTANT:',\n",
       " 'best_response': 'No'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_idx = 0\n",
    "my_dataset[current_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Consider the sentence below in triple backticks and corresponding question. Does the sentence contain enough information to answer the question? Your answer should be either yes or no.\n",
      "\n",
      "Desired format:\n",
      "Answer: <your_answer>\n",
      "Do not print \"Answer:\" again, just what you think the answer is.\n",
      "\n",
      "Sentence:\n",
      "```\n",
      "Nondenominational, evangelical, independent and other churches are on the rise, and constitute a significant part of Protestant Christianity.\n",
      "```\n",
      "Question: What percentage of Christians are Protestant??\n",
      "Answer: ASSISTANT: No\n"
     ]
    }
   ],
   "source": [
    "prompt = my_dataset[current_idx][\"prompt\"]\n",
    "best_response = my_dataset[current_idx][\"best_response\"]\n",
    "\n",
    "print(prompt + \" \" + best_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 10\n",
      "Prompt:\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Consider the sentence below in triple backticks and corresponding question. Does the sentence contain enough information to answer the question? Your answer should be either yes or no.\n",
      "\n",
      "Desired format:\n",
      "Answer: <your_answer>\n",
      "Do not print \"Answer:\" again, just what you think the answer is.\n",
      "\n",
      "Sentence:\n",
      "```\n",
      "Whitehead was apparently not particularly close with his mother, as he never mentioned her in any of his writings, and there is evidence that Whitehead's wife, Evelyn, had a low opinion of her.\n",
      "```\n",
      "Question: What year was Whitehead born??\n",
      "Answer: ASSISTANT:'\n",
      "Best response: No\n",
      "--------------------------------------------------\n",
      "Index: 11\n",
      "Prompt:\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Consider the sentence below in triple backticks and corresponding question. Does the sentence contain enough information to answer the question? Your answer should be either yes or no.\n",
      "\n",
      "Desired format:\n",
      "Answer: <your_answer>\n",
      "Do not print \"Answer:\" again, just what you think the answer is.\n",
      "\n",
      "Sentence:\n",
      "```\n",
      "Further career development is available through the Army Correspondence Course Program.\n",
      "```\n",
      "Question: If the officers want to advance their careers even further, where is that available??\n",
      "Answer: ASSISTANT:'\n",
      "Best response: Yes\n",
      "--------------------------------------------------\n",
      "Index: 12\n",
      "Prompt:\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Consider the sentence below in triple backticks and corresponding question. Does the sentence contain enough information to answer the question? Your answer should be either yes or no.\n",
      "\n",
      "Desired format:\n",
      "Answer: <your_answer>\n",
      "Do not print \"Answer:\" again, just what you think the answer is.\n",
      "\n",
      "Sentence:\n",
      "```\n",
      "Their populations vary between 15,000 (Appenzell Innerrhoden) and 1,253,500 (Zürich), and their area between 37 km2 (14 sq mi) (Basel-Stadt) and 7,105 km2 (2,743 sq mi) (Graubünden).\n",
      "```\n",
      "Question: How large is the smallest canton??\n",
      "Answer: ASSISTANT:'\n",
      "Best response: Yes\n",
      "--------------------------------------------------\n",
      "Index: 13\n",
      "Prompt:\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Consider the sentence below in triple backticks and corresponding question. Does the sentence contain enough information to answer the question? Your answer should be either yes or no.\n",
      "\n",
      "Desired format:\n",
      "Answer: <your_answer>\n",
      "Do not print \"Answer:\" again, just what you think the answer is.\n",
      "\n",
      "Sentence:\n",
      "```\n",
      "In 1903, Porfirio Díaz largely reduced the powers of the municipalities within the Federal District.\n",
      "```\n",
      "Question: When were the powers of the municipalities of Mexico City first reduced??\n",
      "Answer: ASSISTANT:'\n",
      "Best response: Yes\n",
      "--------------------------------------------------\n",
      "Index: 14\n",
      "Prompt:\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Consider the sentence below in triple backticks and corresponding question. Does the sentence contain enough information to answer the question? Your answer should be either yes or no.\n",
      "\n",
      "Desired format:\n",
      "Answer: <your_answer>\n",
      "Do not print \"Answer:\" again, just what you think the answer is.\n",
      "\n",
      "Sentence:\n",
      "```\n",
      "One of the most remote islands in the world, it was for centuries an important stopover for ships sailing to Europe from Asia and South Africa.\n",
      "```\n",
      "Question: How many Boers were taken prisoner during the Second Boer War??\n",
      "Answer: ASSISTANT:'\n",
      "Best response: No\n",
      "--------------------------------------------------\n",
      "Index: 15\n",
      "Prompt:\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Consider the sentence below in triple backticks and corresponding question. Does the sentence contain enough information to answer the question? Your answer should be either yes or no.\n",
      "\n",
      "Desired format:\n",
      "Answer: <your_answer>\n",
      "Do not print \"Answer:\" again, just what you think the answer is.\n",
      "\n",
      "Sentence:\n",
      "```\n",
      "He therefore concluded that the correct procedure would be \"produced by using the earliest \"good\" print as copy-text and inserting into it, from the first edition which contains them, such corrections as appear to us to be derived from the author.\"\n",
      "```\n",
      "Question: What work was the first to see McKerrow use his new method??\n",
      "Answer: ASSISTANT:'\n",
      "Best response: No\n",
      "--------------------------------------------------\n",
      "Index: 16\n",
      "Prompt:\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Consider the sentence below in triple backticks and corresponding question. Does the sentence contain enough information to answer the question? Your answer should be either yes or no.\n",
      "\n",
      "Desired format:\n",
      "Answer: <your_answer>\n",
      "Do not print \"Answer:\" again, just what you think the answer is.\n",
      "\n",
      "Sentence:\n",
      "```\n",
      "Folk songs are commonly ballads which relate heroic deeds and love stories; and religious or devotional songs known as bhajans and banis which are often accompanied by musical instruments like dholak, sitar, and sarangi are also sung.\n",
      "```\n",
      "Question: What kind of song is referred to as a bhajan??\n",
      "Answer: ASSISTANT:'\n",
      "Best response: Yes\n",
      "--------------------------------------------------\n",
      "Index: 17\n",
      "Prompt:\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Consider the sentence below in triple backticks and corresponding question. Does the sentence contain enough information to answer the question? Your answer should be either yes or no.\n",
      "\n",
      "Desired format:\n",
      "Answer: <your_answer>\n",
      "Do not print \"Answer:\" again, just what you think the answer is.\n",
      "\n",
      "Sentence:\n",
      "```\n",
      "According to Eurostat, Greece's largest port by tons of goods transported in 2010 is the port of Aghioi Theodoroi, with 17.38 million tons.\n",
      "```\n",
      "Question: What was Greece's largest port as measured by good transported in 2010??\n",
      "Answer: ASSISTANT:'\n",
      "Best response: Yes\n",
      "--------------------------------------------------\n",
      "Index: 18\n",
      "Prompt:\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Consider the sentence below in triple backticks and corresponding question. Does the sentence contain enough information to answer the question? Your answer should be either yes or no.\n",
      "\n",
      "Desired format:\n",
      "Answer: <your_answer>\n",
      "Do not print \"Answer:\" again, just what you think the answer is.\n",
      "\n",
      "Sentence:\n",
      "```\n",
      "Europeans embraced house, and began booking legendary American house DJs to play at the big clubs, such as Ministry of Sound, whose resident, Justin Berkmann brought in Larry Levan.\n",
      "```\n",
      "Question: who covered and charted the single \"promised land\"??\n",
      "Answer: ASSISTANT:'\n",
      "Best response: No\n",
      "--------------------------------------------------\n",
      "Index: 19\n",
      "Prompt:\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Consider the sentence below in triple backticks and corresponding question. Does the sentence contain enough information to answer the question? Your answer should be either yes or no.\n",
      "\n",
      "Desired format:\n",
      "Answer: <your_answer>\n",
      "Do not print \"Answer:\" again, just what you think the answer is.\n",
      "\n",
      "Sentence:\n",
      "```\n",
      "They wanted to be free of white supervision.\n",
      "```\n",
      "Question: Who wanted to be free from white submission??\n",
      "Answer: ASSISTANT:'\n",
      "Best response: No\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for current_index in range(10, 20):\n",
    "    print(f\"Index: {current_index}\")\n",
    "    print(f\"Prompt:\\n{my_dataset[current_index]['prompt']}'\")\n",
    "    print(f\"Best response: {my_dataset[current_index]['best_response']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# print(f'\\'{my_dataset[current_idx][\"prompt\"]}\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'glue/qnli': 94743})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(my_dataset[\"original_dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_dataset.to_parquet(f\"datasets/qnli_vicuna_ppo_training_{VERSION}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x  3 augustas Domain Users  25K Jul 21 22:46 qnli_vicuna_ppo_training_raw_v1\n",
      "-rw-r--r--  1 augustas Domain Users  19M Jul 21 23:18 qnli_vicuna_ppo_training_v1.parquet\n",
      "drwxr-xr-x  3 augustas Domain Users  33K Jul 21 22:45 qnli_vicuna_train_raw_v1\n",
      "-rw-r--r--  1 augustas Domain Users 2.0M Jul 21 23:02 qnli_vicuna_train_v1.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lah datasets | grep {VERSION} | grep qnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
