{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset_builder, load_dataset\n",
    "\n",
    "from utils import replace_text_with_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/fsx/home-augustas/elk')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ELK_PATH = Path(\"../../../elk/\")\n",
    "ELK_PATH.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/fsx/home-augustas/elk/elk/promptsource', '/fsx/home-augustas/elk']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules = [\n",
    "    ELK_PATH,\n",
    "    ELK_PATH / \"elk\" / \"promptsource\",\n",
    "]\n",
    "\n",
    "for module in modules:\n",
    "    if not str(module) in sys.path:\n",
    "        sys.path.insert(0, str(module.resolve()))\n",
    "\n",
    "sys.path[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from templates import DatasetTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable the logging of the datasets library\n",
    "import datasets\n",
    "\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all datasets used in Burns et al. (2022) (apart from story_cloze)\n",
    "# BURNS_DATASETS = [\n",
    "#     \"ag_news\",\n",
    "#     \"amazon_polarity\",\n",
    "#     \"dbpedia_14\",\n",
    "#     \"glue:qnli\",\n",
    "#     \"imdb\",\n",
    "#     \"piqa\",\n",
    "#     \"super_glue:boolq\",\n",
    "#     \"super_glue:copa\",\n",
    "#     \"super_glue:rte\",\n",
    "# ]\n",
    "BURNS_DATASETS = [\"piqa\"]\n",
    "\n",
    "VERSION = f\"v2\"\n",
    "\n",
    "# SPLIT = \"validation\"\n",
    "SPLIT = \"train\"\n",
    "\n",
    "# These numbers are chosen so that both datasets have\n",
    "# approximately 10k examples in total (probably a bit less for train split)\n",
    "N_PER_DATASET = 17000 if SPLIT == \"train\" else 1550\n",
    "if SPLIT == \"train\":\n",
    "    # assert N_PER_DATASET <= 2490, \"N_PER_DATASET must be <= 2490\"\n",
    "    assert N_PER_DATASET <= 25000, \"N_PER_DATASET must be <= 2490\"\n",
    "else:\n",
    "    assert N_PER_DATASET <= 1838, \"N_PER_DATASET must be <= 1838\"\n",
    "\n",
    "SEED = 42 if SPLIT == \"train\" else 2023\n",
    "SEED"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa\n",
      "split='train'\n",
      "16113\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = {}\n",
    "for dataset_path in BURNS_DATASETS:\n",
    "    print(dataset_path)\n",
    "\n",
    "    # Parse dataset name\n",
    "    dataset_name = None    \n",
    "    if \":\" in dataset_path:\n",
    "        dataset_path, dataset_name = dataset_path.split(\":\")\n",
    "    \n",
    "    \n",
    "    # Get the most validation-like split\n",
    "    available_splits = load_dataset_builder(\n",
    "        dataset_path, name=dataset_name\n",
    "    ).info.splits.keys()\n",
    "    split = \"validation\" if \"validation\" in available_splits else \"test\"\n",
    "    split = split if SPLIT != \"train\" else \"train\"\n",
    "    print(f\"{split=}\")\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\n",
    "        dataset_path, name=dataset_name, split=split,\n",
    "    )\n",
    "\n",
    "    # Get a desired subset of the data\n",
    "    n = N_PER_DATASET if dataset.num_rows > N_PER_DATASET else dataset.num_rows\n",
    "    dataset = dataset.shuffle(seed=SEED).select(range(n))\n",
    "\n",
    "    print(dataset.num_rows)\n",
    "\n",
    "    key = f\"{dataset_path}/{dataset_name}\" if dataset_name else dataset_path\n",
    "    dataset_dict[key] = dataset\n",
    "\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16113"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of examples in total in all datasets\n",
    "sum([dataset.num_rows for dataset in dataset_dict.values()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([len(Counter(dataset[\"label\"])) > 1 for dataset in dataset_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa: 16113\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(f\"{dataset_name}: {dataset.num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa: 2\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(f\"{dataset_name}: {len(Counter(dataset['label']))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa\n",
      "10000 6113\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_dataset_dict = {}\n",
    "ppo_dataset_dict = {}\n",
    "\n",
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(dataset_name)\n",
    "    splits = dataset.train_test_split(train_size=10_000, seed=SEED)\n",
    "    # print(splits)\n",
    "    print(splits[\"train\"].num_rows, splits[\"test\"].num_rows)\n",
    "\n",
    "    train_dataset_dict[dataset_name] = splits[\"train\"]\n",
    "    ppo_dataset_dict[dataset_name] = splits[\"test\"]\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "6113\n"
     ]
    }
   ],
   "source": [
    "print(sum(dataset.num_rows for dataset in train_dataset_dict.values()))\n",
    "print(sum(dataset.num_rows for dataset in ppo_dataset_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6df10c452aa4609b55a225f0ed81d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# datasets.DatasetDict(train_dataset_dict).save_to_disk(\n",
    "#     # f\"datasets/burns_datasets_VINC_train_raw_{VERSION}\"\n",
    "#     # f\"datasets/burns_datasets_VINC_imdb_train_raw_{VERSION}\"\n",
    "#     f\"datasets/wrapped_piqa_train_raw_{VERSION}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1eaf9d7d8d4061a7ff3c0a063545ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# datasets.DatasetDict(ppo_dataset_dict).save_to_disk(\n",
    "#     # f\"datasets/burns_datasets_VINC_ppo_training_raw_{VERSION}\"\n",
    "#     # f\"datasets/burns_datasets_VINC_imdb_ppo_training_raw_{VERSION}\"\n",
    "#     f\"datasets/wrapped_piqa_ppo_training_raw_{VERSION}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x  3 augustas Domain Users 1.0K Jul 24 20:31 wrapped_piqa_ppo_training_raw_v2\n",
      "drwxr-xr-x  3 augustas Domain Users  33K Jul 24 20:31 wrapped_piqa_train_raw_v2\n"
     ]
    }
   ],
   "source": [
    "# !ls -lah datasets | grep {VERSION}\n",
    "# !ls -lah datasets | grep {VERSION} | grep imdb\n",
    "!ls -lah datasets | grep {VERSION} | grep piqa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    piqa: Dataset({\n",
       "        features: ['goal', 'sol1', 'sol2', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set dataset_dict to test_dataset\n",
    "dataset_dict = datasets.DatasetDict.load_from_disk(\n",
    "#     # f\"datasets/burns_datasets_VINC_train_raw_{VERSION}\"\n",
    "    # f\"datasets/burns_datasets_VINC_imdb_train_raw_{VERSION}\"\n",
    "    f\"datasets/wrapped_piqa_train_raw_{VERSION}\"\n",
    ")\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dataset.num_rows for dataset in dataset_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa: 2\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(f\"{dataset_name}: {len(Counter(dataset['label']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_template_dict = {}\n",
    "\n",
    "for dataset_path in dataset_dict.keys():\n",
    "    dataset_templates = DatasetTemplates(dataset_path)\n",
    "\n",
    "    dataset_templates.templates = {\n",
    "        x.name: x for x in dataset_templates.templates.values() if x.get_answer_choices_list(dataset_dict[dataset_path][0]) is not None\n",
    "    }\n",
    "\n",
    "    dataset_template_dict[dataset_path] = dataset_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa: 7\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "    print(f\"{dataset_name}: {len(dataset_templates.templates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa: 6\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in dataset_template_dict:\n",
    "    good_templates = {\n",
    "        name: x for name, x in dataset_template_dict[dataset_name].templates.items() if x.metadata.choices_in_prompt\n",
    "    }\n",
    "    dataset_template_dict[dataset_name].templates = good_templates\n",
    "    print(f\"{dataset_name}: {len(good_templates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa: 6\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "    print(f\"{dataset_name}: {len(dataset_templates.templates)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what_is_the_correct_ending\n",
      "---------------------------------\n",
      "pick_correct_choice_with_choice_given_before_goal\n",
      "---------------------------------\n",
      "pick_correct_choice_index\n",
      "---------------------------------\n",
      "finish_sentence_with_correct_choice\n",
      "---------------------------------\n",
      "choose the most appropriate solution\n",
      "---------------------------------\n",
      "Does this solution make sense? sol1\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"piqa\"\n",
    "\n",
    "for template_name, template in dataset_template_dict[dataset_name].templates.items():\n",
    "    print(template_name)\n",
    "    # print(template.jinja)\n",
    "    # print(template.metadata.choices_in_prompt)\n",
    "    # print(dataset_dict[dataset_name][0])\n",
    "    q, a = template.apply(\n",
    "        dataset_dict[dataset_name][0]\n",
    "    )\n",
    "    # print(q == q.strip())\n",
    "    # print(a == a.strip())\n",
    "    # print(\" \".join([q, a.strip()]))\n",
    "    # print(\" \".join([q, a]))\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form the dataset for the chosen split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "#     print(dataset_name)\n",
    "#     for template_name, template in dataset_templates.templates.items():\n",
    "#         print(f\"{template_name}: {template.get_fixed_answer_choices_list()}\")\n",
    "\n",
    "#     print(\"---------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "#     print(dataset_name)\n",
    "#     for template_name, template in dataset_templates.templates.items():\n",
    "#         print(f\"{template_name}: {template.get_answer_choices_list(dataset_dict[dataset_name][0])}\")\n",
    "\n",
    "#     print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa: 6\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "    print(f\"{dataset_name}: {len(dataset_templates.templates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.1 s, sys: 0 ns, total: 23.1 s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(SEED)\n",
    "\n",
    "ALLOWED_KEYS = [\"text\", \"label\", \"original_dataset\", \"template_name\"]\n",
    "\n",
    "new_dataset = []\n",
    "\n",
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(dataset_name)\n",
    "\n",
    "    for idx, entry in enumerate(dataset):\n",
    "        new_entry = entry.copy()\n",
    "        \n",
    "        # In case we need to know which dataset the entry came from\n",
    "        new_entry[\"original_dataset\"] = dataset_name\n",
    "\n",
    "        # Sample a random template\n",
    "        template_name = random.choice(\n",
    "            list(dataset_template_dict[dataset_name].templates.keys())\n",
    "        )\n",
    "        template = dataset_template_dict[dataset_name].templates[template_name]\n",
    "        new_entry[\"template_name\"] = template_name\n",
    "\n",
    "        # Whether the sample will be truthful or not\n",
    "        is_truthful = random.choice([True, False])\n",
    "\n",
    "        # If not truthful, sample a random incorrect label\n",
    "        if not is_truthful and len(dataset.features[\"label\"].names) > 2:\n",
    "            label_mappping = dataset.features[\"label\"] # [class 1, class 2, ...]\n",
    "            all_label_ids = [label_mappping.str2int(x) for x in label_mappping.names] # [0, 1, ...]\n",
    "            incorrect_label_id = all_label_ids[(1 - new_entry[\"label\"]) % len(all_label_ids)]\n",
    "            incorrect_label = template.get_fixed_answer_choices_list()[incorrect_label_id]\n",
    "            \n",
    "        \n",
    "        # Apply the template\n",
    "        if is_truthful:\n",
    "            # new_text = \" \".join(template.apply(new_entry))\n",
    "            q, a = template.apply(new_entry)\n",
    "            new_text = \" \".join([q.rstrip(), a.strip()])\n",
    "        elif not is_truthful and len(dataset.features[\"label\"].names) > 2:\n",
    "            q, a = template.apply(new_entry)\n",
    "            incorrect_label = replace_text_with_whitespace(a, incorrect_label)\n",
    "            new_text = \" \".join([q, incorrect_label])\n",
    "        else:\n",
    "            # Untruthful binary case\n",
    "            new_entry[\"label\"] = 1 - new_entry[\"label\"]\n",
    "            # new_text = \" \".join(template.apply(new_entry))\n",
    "            q, a = template.apply(new_entry)\n",
    "            new_text = \" \".join([q.rstrip(), a.strip()])\n",
    "\n",
    "        new_entry[\"text\"] = new_text\n",
    "\n",
    "        # We can now change the label to whether the sample is truthful or not\n",
    "        new_entry[\"label\"] = int(is_truthful)\n",
    "\n",
    "        # Remove all other keys\n",
    "        new_entry = { k: v for k, v in new_entry.items() if k in ALLOWED_KEYS }\n",
    "\n",
    "        # Append to the new dataset\n",
    "        new_dataset.append(new_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'original_dataset', 'template_name', 'text'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset = datasets.Dataset.from_list(new_dataset)\n",
    "my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'original_dataset': 'piqa',\n",
       " 'template_name': 'Does this solution make sense? sol1',\n",
       " 'text': 'Does this phrase make sense?\\nWhat service to turn dough on when making biscuits? turn the dough onto a lightly floured work surface and continue kneading until everything comes together.\\nAnswer with Yes or No Yes'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_idx = 0\n",
    "my_dataset[current_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=0\n",
      "Does this phrase make sense?\n",
      "To grill hamburger safely and tastily leave meat under the grill until you can no longer see any blood. Remove.\n",
      "Answer with Yes or No Yes\n",
      "---------------------------------\n",
      "label=0\n",
      "Solution 1: Sprinkle your chosen cheese on top of the tortilla chips and place in the oven and set to broil.  Leave them in the oven until cheese has melted and the chips are warm.\n",
      "Solution 2: Sprinkle your chosen cheese on top of the tortilla chips and place on the stove and set high.  Leave them on the stove until cheese has melted and the chips are warm.\n",
      "\n",
      "Goal: How do you melt cheese on nachos?\n",
      "\n",
      "Given the goal, what is the correct solution?\n",
      "\n",
      "Answer by copying the correct solution Sprinkle your chosen cheese on top of the tortilla chips and place on the stove and set high.  Leave them on the stove until cheese has melted and the chips are warm.\n",
      "---------------------------------\n",
      "label=0\n",
      "Given a goal and 2 solutions, choose the most appropriate solution.\n",
      "Goal: How can I make a cool ink stamp quickly?\n",
      "- Solution 1: Cut a pod of boiled okra horizontally, dip in ink and stamp on paper for a cool design.\n",
      "- Solution 2: Cut a pod of okra horizontally, dip in ink and stamp on paper for a cool design.\n",
      "\n",
      "Answer by returning either Solution 1 or Solution 2 Solution 1\n",
      "---------------------------------\n",
      "label=1\n",
      "Goal: How to put batter in a loaf pan\n",
      "\n",
      "Which is the correct ending?\n",
      "- Scrape the batter into the prepared loaf pan and smooth the bottom with the back of a spoon.\n",
      "- Scrape the batter into the prepared loaf pan and smooth the top with the back of a spoon.\n",
      "\n",
      "Answer: Scrape the batter into the prepared loaf pan and smooth the top with the back of a spoon.\n",
      "---------------------------------\n",
      "label=0\n",
      "Does this phrase make sense?\n",
      "Prevent shirt buttons from falling. dab a small amount of white out over threads.\n",
      "Answer with Yes or No Yes\n",
      "---------------------------------\n",
      "label=0\n",
      "Sentence: how do you use the mats at a gym?\n",
      "\n",
      "Choice 1: they are there for safety if you fall.\n",
      "\n",
      "Choice 2: they are there for you to jump on to warm up.\n",
      "\n",
      "What is the index of the correct choice for ending for the sentence?\n",
      "\n",
      "Answer: 2\n",
      "---------------------------------\n",
      "label=1\n",
      "Solution 1: Jack up the car 12 inches just so you can get under. Grab a pan to catch the oil in. First unscrew oil filter let oil drip out. Unscrew the oil pan plug screw. Leave some dirty oil in the car for lubrication. Put back on old oil filter and oil drain plug. Put recommend oil in car. Put car on the ground and don't start the car for 15 minutes. Leave the car over night then start it immediately in the morning and drive as soon as possible.\n",
      "Solution 2: First jack the car up so its safe enough to get under. Grab a pan to catch the oil in. Next, find the bolt to the oil pan and unscrew it carefully. Let the oil drip out. Locate the oil filter and carefully unscrew it. Let all the oil drip out. Clean screw with rag. Put oil screw back on oil pan and install a new oil filter. Put recommended amount of oil in car. Double check dipstick and put car back on the ground. Let motor run for 15 minutes until warm.\n",
      "\n",
      "Goal: How to change engine oil.\n",
      "\n",
      "Given the goal, what is the correct solution?\n",
      "\n",
      "Answer by copying the correct solution First jack the car up so its safe enough to get under. Grab a pan to catch the oil in. Next, find the bolt to the oil pan and unscrew it carefully. Let the oil drip out. Locate the oil filter and carefully unscrew it. Let all the oil drip out. Clean screw with rag. Put oil screw back on oil pan and install a new oil filter. Put recommended amount of oil in car. Double check dipstick and put car back on the ground. Let motor run for 15 minutes until warm.\n",
      "---------------------------------\n",
      "label=1\n",
      "Sentence: How do I get all the water out of a garden hose before storing it?\n",
      "\n",
      "Choice 1: Pick up one end of the hose and lift it over your head and then continue feeding the hose through your hands and let the hose that has been over your head float to the ground to get all the water out of the hose.\n",
      "\n",
      "Choice 2: Pick up one end of the hose and lift it over your head and then continue feeding the hose through your hands and let the hose that has been over your head fall to the ground to get all the water out of the hose.\n",
      "\n",
      "What is the index of the correct choice for ending for the sentence?\n",
      "\n",
      "Answer: 2\n",
      "---------------------------------\n",
      "label=0\n",
      "Goal: To make rock candy\n",
      "\n",
      "Which is the correct ending?\n",
      "- Bring one cup of water to a boil in a saucepan and dissolve a quarter cup of white sugar into it, until no more will dissolve. Still eating and stirring, keep adding more sugar, stirring each time until it dissolves, until you've added three cups of sugar. Remove from the heat and allow to cool. Dip a bamboo skewer into the solution, remove and roll in sugar. Pour the solution into a jar. Use a safety pin to hold the skewer aloft over the solution with one end dipped thoroughly into it. Wait seven days for the rock candy to farm.\n",
      "- Bring one cup of water to a boil in a saucepan and dissolve a quarter cup of white sugar into it, until no more will dissolve. Still eating and stirring, keep adding more sugar, stirring each time until it dissolves, until you've added three cups of sugar. Remove from the heat and allow to cool. Dip a bamboo skewer into the solution, remove and roll in sugar. Pour the solution into a jar. Use a clothes pin to hold the skewer aloft over the solution with one end dipped thoroughly into it. Wait seven days for the rock candy to farm.\n",
      "\n",
      "Answer: Bring one cup of water to a boil in a saucepan and dissolve a quarter cup of white sugar into it, until no more will dissolve. Still eating and stirring, keep adding more sugar, stirring each time until it dissolves, until you've added three cups of sugar. Remove from the heat and allow to cool. Dip a bamboo skewer into the solution, remove and roll in sugar. Pour the solution into a jar. Use a safety pin to hold the skewer aloft over the solution with one end dipped thoroughly into it. Wait seven days for the rock candy to farm.\n",
      "---------------------------------\n",
      "label=0\n",
      "Goal: How to clean a knife\n",
      "\n",
      "Which is the correct ending?\n",
      "- Use soap and water, clean from behind so you do not cut your fingers or hands.\n",
      "- Place the knife in a pot of boiling water for 5 minutes to kill any bacteria.\n",
      "\n",
      "Answer: Place the knife in a pot of boiling water for 5 minutes to kill any bacteria.\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "for current_index in range(10, 20):\n",
    "    print(f\"label={my_dataset[current_index]['label']}\")\n",
    "    print(my_dataset[current_index][\"text\"])\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({1: 5039, 0: 4961}), Counter({'piqa': 10000}))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counter({1: 5055, 0: 4945}\n",
    "# Counter({1: 5021, 0: 4979}\n",
    "# Counter({0: 5031, 1: 4969}\n",
    "# Counter({0: 5067, 1: 4933}\n",
    "# Counter({0: 5108, 1: 4892}\n",
    "Counter(my_dataset[\"label\"]), Counter(my_dataset[\"original_dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207df4d58eaa4da195d657b95d899ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4256068"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my_dataset.to_parquet(f\"datasets/burns_datasets_VINC_{SPLIT}_{VERSION}.parquet\")\n",
    "# my_dataset.to_parquet(f\"datasets/burns_datasets_VINC_imdb_{SPLIT}_{VERSION}.parquet\")\n",
    "# my_dataset.to_parquet(f\"datasets/wrapped_piqa_{SPLIT}_{VERSION}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x  3 augustas Domain Users  25K Jul 24 20:31 wrapped_piqa_ppo_training_raw_v2\n",
      "-rw-r--r--  1 augustas Domain Users 972K Jul 24 21:26 wrapped_piqa_ppo_training_v2.parquet\n",
      "drwxr-xr-x  3 augustas Domain Users  33K Jul 24 20:31 wrapped_piqa_train_raw_v2\n",
      "-rw-r--r--  1 augustas Domain Users 1.4M Jul 24 21:36 wrapped_piqa_train_v2.parquet\n"
     ]
    }
   ],
   "source": [
    "# !ls -lah datasets | grep {VERSION}\n",
    "# !ls -lah datasets | grep {VERSION} | grep imdb\n",
    "!ls -lah datasets | grep {VERSION} | grep piqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
