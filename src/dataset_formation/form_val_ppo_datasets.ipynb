{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset_builder, load_dataset\n",
    "\n",
    "from utils import replace_text_with_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/fsx/home-augustas/elk')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ELK_PATH = Path(\"../../../elk/\")\n",
    "ELK_PATH.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/fsx/home-augustas/elk/elk/promptsource', '/fsx/home-augustas/elk']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules = [\n",
    "    ELK_PATH,\n",
    "    ELK_PATH / \"elk\" / \"promptsource\",\n",
    "]\n",
    "\n",
    "for module in modules:\n",
    "    if not str(module) in sys.path:\n",
    "        sys.path.insert(0, str(module.resolve()))\n",
    "\n",
    "sys.path[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from templates import DatasetTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable the logging of the datasets library\n",
    "import datasets\n",
    "\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2023"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all datasets used in Burns et al. (2022) (apart from story_cloze)\n",
    "BURNS_DATASETS = [\n",
    "    \"ag_news\",\n",
    "    \"amazon_polarity\",\n",
    "    \"dbpedia_14\",\n",
    "    \"glue:qnli\",\n",
    "    \"imdb\",\n",
    "    \"piqa\",\n",
    "    \"super_glue:boolq\",\n",
    "    \"super_glue:copa\",\n",
    "    \"super_glue:rte\",\n",
    "]\n",
    "# BURNS_DATASETS = [\"ag_news\"]\n",
    "# BURNS_DATASETS = [\n",
    "#     \"ag_news\",\n",
    "#     \"amazon_polarity\",\n",
    "#     \"dbpedia_14\",\n",
    "#     \"glue:qnli\",\n",
    "#     \"imdb\",\n",
    "#     \"super_glue:boolq\",\n",
    "#     \"super_glue:copa\",\n",
    "#     \"super_glue:rte\",\n",
    "# ]\n",
    "# BURNS_DATASETS = [\n",
    "#     \"ag_news\",\n",
    "#     \"amazon_polarity\",\n",
    "#     \"dbpedia_14\",\n",
    "#     \"imdb\",\n",
    "#     \"super_glue:boolq\",\n",
    "#     \"super_glue:copa\",\n",
    "# ]\n",
    "\n",
    "VERSION = \"v4\"\n",
    "\n",
    "# SPLIT = \"train\"\n",
    "SPLIT = \"validation\"\n",
    "\n",
    "# These numbers are chosen so that both datasets have\n",
    "# approximately 10k examples in total (probably a bit less for train split)\n",
    "# N_PER_DATASET = 1000 if SPLIT == \"train\" else 1000\n",
    "N_PER_DATASET = 1200 if SPLIT == \"train\" else 1550\n",
    "if SPLIT == \"train\":\n",
    "    assert N_PER_DATASET <= 2490, \"N_PER_DATASET must be <= 2490\"\n",
    "else:\n",
    "    assert N_PER_DATASET <= 1838, \"N_PER_DATASET must be <= 1838\"\n",
    "\n",
    "SEED = 42 if SPLIT == \"train\" else 2023\n",
    "SEED"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_news\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split='test'\n",
      "1550\n",
      "-----------------------------------\n",
      "amazon_polarity\n",
      "split='test'\n",
      "1550\n",
      "-----------------------------------\n",
      "dbpedia_14\n",
      "split='test'\n",
      "1550\n",
      "-----------------------------------\n",
      "glue:qnli\n",
      "split='validation'\n",
      "1550\n",
      "-----------------------------------\n",
      "imdb\n",
      "split='test'\n",
      "1550\n",
      "-----------------------------------\n",
      "piqa\n",
      "split='validation'\n",
      "1550\n",
      "-----------------------------------\n",
      "super_glue:boolq\n",
      "split='validation'\n",
      "1550\n",
      "-----------------------------------\n",
      "super_glue:copa\n",
      "split='validation'\n",
      "100\n",
      "-----------------------------------\n",
      "super_glue:rte\n",
      "split='validation'\n",
      "277\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = {}\n",
    "for dataset_path in BURNS_DATASETS:\n",
    "    print(dataset_path)\n",
    "\n",
    "    # Parse dataset name\n",
    "    dataset_name = None    \n",
    "    if \":\" in dataset_path:\n",
    "        dataset_path, dataset_name = dataset_path.split(\":\")\n",
    "    \n",
    "    \n",
    "    # Get the most validation-like split\n",
    "    available_splits = load_dataset_builder(\n",
    "        dataset_path, name=dataset_name\n",
    "    ).info.splits.keys()\n",
    "    split = \"validation\" if \"validation\" in available_splits else \"test\"\n",
    "    split = split if SPLIT != \"train\" else \"train\"\n",
    "    print(f\"{split=}\")\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\n",
    "        dataset_path, name=dataset_name, split=split,\n",
    "    )\n",
    "\n",
    "    # Get a desired subset of the data\n",
    "    n = N_PER_DATASET if dataset.num_rows > N_PER_DATASET else dataset.num_rows\n",
    "    dataset = dataset.shuffle(seed=42).select(range(n))\n",
    "\n",
    "    print(dataset.num_rows)\n",
    "\n",
    "    key = f\"{dataset_path}/{dataset_name}\" if dataset_name else dataset_path\n",
    "    dataset_dict[key] = dataset\n",
    "\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11227"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of examples in total in all datasets\n",
    "sum([dataset.num_rows for dataset in dataset_dict.values()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([len(Counter(dataset[\"label\"])) > 1 for dataset in dataset_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_news: 1550\n",
      "amazon_polarity: 1550\n",
      "dbpedia_14: 1550\n",
      "glue/qnli: 1550\n",
      "imdb: 1550\n",
      "piqa: 1550\n",
      "super_glue/boolq: 1550\n",
      "super_glue/copa: 100\n",
      "super_glue/rte: 277\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(f\"{dataset_name}: {dataset.num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_news: 4\n",
      "amazon_polarity: 2\n",
      "dbpedia_14: 14\n",
      "glue/qnli: 2\n",
      "imdb: 2\n",
      "piqa: 2\n",
      "super_glue/boolq: 2\n",
      "super_glue/copa: 2\n",
      "super_glue/rte: 2\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(f\"{dataset_name}: {len(Counter(dataset['label']))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_news\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1380 170\n",
      "-----------------------------------\n",
      "amazon_polarity\n",
      "1380 170\n",
      "-----------------------------------\n",
      "dbpedia_14\n",
      "1380 170\n",
      "-----------------------------------\n",
      "glue/qnli\n",
      "1380 170\n",
      "-----------------------------------\n",
      "imdb\n",
      "1380 170\n",
      "-----------------------------------\n",
      "piqa\n",
      "1380 170\n",
      "-----------------------------------\n",
      "super_glue/boolq\n",
      "1380 170\n",
      "-----------------------------------\n",
      "super_glue/copa\n",
      "89 11\n",
      "-----------------------------------\n",
      "super_glue/rte\n",
      "246 31\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "ppo_dataset_dict = {}\n",
    "test_dataset_dict = {}\n",
    "\n",
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(dataset_name)\n",
    "    # test_size = 225 if dataset.num_rows > 225 else 10 # specialized for COPA\n",
    "    splits = dataset.train_test_split(test_size=0.1093, seed=42)\n",
    "    # print(splits)\n",
    "    print(splits[\"train\"].num_rows, splits[\"test\"].num_rows)\n",
    "\n",
    "    ppo_dataset_dict[dataset_name] = splits[\"train\"]\n",
    "    test_dataset_dict[dataset_name] = splits[\"test\"]\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9995\n",
      "1232\n"
     ]
    }
   ],
   "source": [
    "print(sum(dataset.num_rows for dataset in ppo_dataset_dict.values()))\n",
    "print(sum(dataset.num_rows for dataset in test_dataset_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets.DatasetDict(ppo_dataset_dict).save_to_disk(\n",
    "#     f\"datasets/burns_datasets_VINC_ppo_training_raw_{VERSION}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets.DatasetDict(test_dataset_dict).save_to_disk(\n",
    "#     f\"datasets/burns_datasets_VINC_raw_{VERSION}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x  9 augustas Domain Users  25K Jun 29 11:14 burns_datasets_VINC_ppo_training_raw_v4\n",
      "drwxr-xr-x  9 augustas Domain Users  33K Jun 29 11:14 burns_datasets_VINC_raw_v4\n",
      "-rw-r--r--  1 augustas Domain Users 2.8M Jun 29 10:48 burns_datasets_VINC_train_v4.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lah datasets | grep {VERSION}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    ag_news: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 170\n",
       "    })\n",
       "    amazon_polarity: Dataset({\n",
       "        features: ['label', 'title', 'content'],\n",
       "        num_rows: 170\n",
       "    })\n",
       "    dbpedia_14: Dataset({\n",
       "        features: ['label', 'title', 'content'],\n",
       "        num_rows: 170\n",
       "    })\n",
       "    glue/qnli: Dataset({\n",
       "        features: ['question', 'sentence', 'label', 'idx'],\n",
       "        num_rows: 170\n",
       "    })\n",
       "    imdb: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 170\n",
       "    })\n",
       "    piqa: Dataset({\n",
       "        features: ['goal', 'sol1', 'sol2', 'label'],\n",
       "        num_rows: 170\n",
       "    })\n",
       "    super_glue/boolq: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 170\n",
       "    })\n",
       "    super_glue/copa: Dataset({\n",
       "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'label'],\n",
       "        num_rows: 11\n",
       "    })\n",
       "    super_glue/rte: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
       "        num_rows: 31\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set dataset_dict to test_dataset\n",
    "dataset_dict = datasets.DatasetDict.load_from_disk(\n",
    "    f\"datasets/burns_datasets_VINC_raw_{VERSION}\"\n",
    ")\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1232"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dataset.num_rows for dataset in dataset_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_news: 4\n",
      "amazon_polarity: 2\n",
      "dbpedia_14: 14\n",
      "glue/qnli: 2\n",
      "imdb: 2\n",
      "piqa: 2\n",
      "super_glue/boolq: 2\n",
      "super_glue/copa: 2\n",
      "super_glue/rte: 2\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(f\"{dataset_name}: {len(Counter(dataset['label']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_template_dict = {}\n",
    "\n",
    "for dataset_path in dataset_dict.keys():\n",
    "    dataset_templates = DatasetTemplates(dataset_path)\n",
    "\n",
    "    dataset_templates.templates = {\n",
    "        x.name: x for x in dataset_templates.templates.values() if x.get_answer_choices_list(dataset_dict[dataset_path][0]) is not None\n",
    "    }\n",
    "\n",
    "    dataset_template_dict[dataset_path] = dataset_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_news: 15\n",
      "amazon_polarity: 11\n",
      "dbpedia_14: 11\n",
      "glue/qnli: 5\n",
      "imdb: 13\n",
      "piqa: 7\n",
      "super_glue/boolq: 10\n",
      "super_glue/copa: 9\n",
      "super_glue/rte: 11\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "    print(f\"{dataset_name}: {len(dataset_templates.templates)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"ag_news\"\n",
    "\n",
    "# for template_name, template in dataset_template_dict[dataset_name].templates.items():\n",
    "#     # print(template_name)\n",
    "#     # print(dataset_dict[dataset_name][0])\n",
    "#     q, a = template.apply(\n",
    "#         dataset_dict[dataset_name][0]\n",
    "#     )\n",
    "#     # print(q == q.strip())\n",
    "#     # print(a == a.strip())\n",
    "#     # print(\" \".join([q, a.strip()]))\n",
    "#     print(\" \".join([q, a]))\n",
    "#     print(\"---------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form the dataset for the chosen split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "#     print(dataset_name)\n",
    "#     for template_name, template in dataset_templates.templates.items():\n",
    "#         print(f\"{template_name}: {template.get_fixed_answer_choices_list()}\")\n",
    "\n",
    "#     print(\"---------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "#     print(dataset_name)\n",
    "#     for template_name, template in dataset_templates.templates.items():\n",
    "#         print(f\"{template_name}: {template.get_answer_choices_list(dataset_dict[dataset_name][0])}\")\n",
    "\n",
    "#     print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_news: 15\n",
      "amazon_polarity: 11\n",
      "dbpedia_14: 11\n",
      "glue/qnli: 5\n",
      "imdb: 13\n",
      "piqa: 7\n",
      "super_glue/boolq: 10\n",
      "super_glue/copa: 9\n",
      "super_glue/rte: 11\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "    print(f\"{dataset_name}: {len(dataset_templates.templates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_news\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_polarity\n",
      "dbpedia_14\n",
      "glue/qnli\n",
      "imdb\n",
      "piqa\n",
      "super_glue/boolq\n",
      "super_glue/copa\n",
      "super_glue/rte\n",
      "CPU times: user 2.55 s, sys: 185 ms, total: 2.74 s\n",
      "Wall time: 2.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(SEED)\n",
    "\n",
    "ALLOWED_KEYS = [\"text\", \"label\", \"original_dataset\", \"template_name\"]\n",
    "\n",
    "new_dataset = []\n",
    "\n",
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(dataset_name)\n",
    "\n",
    "    for idx, entry in enumerate(dataset):\n",
    "        new_entry = entry.copy()\n",
    "        \n",
    "        # In case we need to know which dataset the entry came from\n",
    "        new_entry[\"original_dataset\"] = dataset_name\n",
    "\n",
    "        # Sample a random template\n",
    "        template_name = random.choice(\n",
    "            list(dataset_template_dict[dataset_name].templates.keys())\n",
    "        )\n",
    "        template = dataset_template_dict[dataset_name].templates[template_name]\n",
    "        new_entry[\"template_name\"] = template_name\n",
    "\n",
    "        # Whether the sample will be truthful or not\n",
    "        is_truthful = random.choice([True, False])\n",
    "\n",
    "        # If not truthful, sample a random incorrect label\n",
    "        if not is_truthful and len(dataset.features[\"label\"].names) > 2:\n",
    "            label_mappping = dataset.features[\"label\"] # [class 1, class 2, ...]\n",
    "            all_label_ids = [label_mappping.str2int(x) for x in label_mappping.names] # [0, 1, ...]\n",
    "            incorrect_label_id = all_label_ids[(1 - new_entry[\"label\"]) % len(all_label_ids)]\n",
    "            incorrect_label = template.get_fixed_answer_choices_list()[incorrect_label_id]\n",
    "            \n",
    "        \n",
    "        # Apply the template\n",
    "        if is_truthful:\n",
    "            new_text = \" \".join(template.apply(new_entry))\n",
    "        elif not is_truthful and len(dataset.features[\"label\"].names) > 2:\n",
    "            q, a = template.apply(new_entry)\n",
    "            incorrect_label = replace_text_with_whitespace(a, incorrect_label)\n",
    "            new_text = \" \".join([q, incorrect_label])\n",
    "        else:\n",
    "            # Untruthful binary case\n",
    "            new_entry[\"label\"] = 1 - new_entry[\"label\"]\n",
    "            new_text = \" \".join(template.apply(new_entry))\n",
    "\n",
    "        new_entry[\"text\"] = new_text\n",
    "\n",
    "        # We can now change the label to whether the sample is truthful or not\n",
    "        new_entry[\"label\"] = int(is_truthful)\n",
    "\n",
    "        # Remove all other keys\n",
    "        new_entry = { k: v for k, v in new_entry.items() if k in ALLOWED_KEYS }\n",
    "\n",
    "        # Append to the new dataset\n",
    "        new_dataset.append(new_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'original_dataset', 'template_name'],\n",
       "    num_rows: 1232\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset = datasets.Dataset.from_list(new_dataset)\n",
    "my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Consider the following example: ''' Bush Backs US Tariffs on Shrimp Foreign shrimp producers have denied they are selling shrimp at artificially low prices as a way to win a larger share of the US market. ''' Choice 1: Business. Choice 2: Science and technology.Between choice 1 and choice 2, what is the topic of this example? Science and technology\",\n",
       " 'label': 0,\n",
       " 'original_dataset': 'ag_news',\n",
       " 'template_name': 'burns_2'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_idx = 0\n",
    "my_dataset[current_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider the following example: ''' Bush Backs US Tariffs on Shrimp Foreign shrimp producers have denied they are selling shrimp at artificially low prices as a way to win a larger share of the US market. ''' Choice 1: Business. Choice 2: Science and technology.Between choice 1 and choice 2, what is the topic of this example? Science and technology\n"
     ]
    }
   ],
   "source": [
    "print(my_dataset[current_idx][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0: 623, 1: 609}),\n",
       " Counter({'ag_news': 170,\n",
       "          'amazon_polarity': 170,\n",
       "          'dbpedia_14': 170,\n",
       "          'glue/qnli': 170,\n",
       "          'imdb': 170,\n",
       "          'piqa': 170,\n",
       "          'super_glue/boolq': 170,\n",
       "          'super_glue/rte': 31,\n",
       "          'super_glue/copa': 11}))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(my_dataset[\"label\"]), Counter(my_dataset[\"original_dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_dataset.to_parquet(f\"datasets/burns_datasets_VINC_{SPLIT}_{VERSION}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x  9 augustas Domain Users  25K Jun 29 11:14 burns_datasets_VINC_ppo_training_raw_v4\n",
      "drwxr-xr-x  9 augustas Domain Users  33K Jun 29 11:14 burns_datasets_VINC_raw_v4\n",
      "-rw-r--r--  1 augustas Domain Users 2.8M Jun 29 10:48 burns_datasets_VINC_train_v4.parquet\n",
      "-rw-r--r--  1 augustas Domain Users 380K Jun 29 11:18 burns_datasets_VINC_validation_v4.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lah datasets | grep {VERSION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
