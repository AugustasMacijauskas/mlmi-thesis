{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset_builder, load_dataset\n",
    "\n",
    "from utils import replace_text_with_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/fsx/home-augustas/elk')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ELK_PATH = Path(\"../../../elk/\")\n",
    "ELK_PATH.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/fsx/home-augustas/elk/elk/promptsource', '/fsx/home-augustas/elk']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules = [\n",
    "    ELK_PATH,\n",
    "    ELK_PATH / \"elk\" / \"promptsource\",\n",
    "]\n",
    "\n",
    "for module in modules:\n",
    "    if not str(module) in sys.path:\n",
    "        sys.path.insert(0, str(module.resolve()))\n",
    "\n",
    "sys.path[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from templates import DatasetTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable the logging of the datasets library\n",
    "import datasets\n",
    "\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all datasets used in Burns et al. (2022) (apart from story_cloze)\n",
    "# BURNS_DATASETS = [\n",
    "#     \"ag_news\",\n",
    "#     \"amazon_polarity\",\n",
    "#     \"dbpedia_14\",\n",
    "#     \"glue:qnli\",\n",
    "#     \"imdb\",\n",
    "#     \"piqa\",\n",
    "#     \"super_glue:boolq\",\n",
    "#     \"super_glue:copa\",\n",
    "#     \"super_glue:rte\",\n",
    "# ]\n",
    "BURNS_DATASETS = [\"ag_news\"]\n",
    "\n",
    "SPLIT = \"validation\"\n",
    "\n",
    "# These numbers are chosen so that both datasets have\n",
    "# approximately 10k examples in total\n",
    "N_PER_DATASET = 1200 if SPLIT == \"train\" else 1800\n",
    "N_PER_DATASET = 1200 if SPLIT == \"train\" else 7600\n",
    "# if SPLIT == \"train\":\n",
    "#     assert N_PER_DATASET <= 2490, \"N_PER_DATASET must be <= 2490 for train\"\n",
    "# else:\n",
    "#     assert N_PER_DATASET <= 1838, \"N_PER_DATASET must be <= 1838 for validation\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_news\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split='test'\n",
      "7600\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = {}\n",
    "for dataset_path in BURNS_DATASETS:\n",
    "    print(dataset_path)\n",
    "\n",
    "    # Parse dataset name\n",
    "    dataset_name = None    \n",
    "    if \":\" in dataset_path:\n",
    "        dataset_path, dataset_name = dataset_path.split(\":\")\n",
    "    \n",
    "    \n",
    "    # Get the most validation-like split\n",
    "    available_splits = load_dataset_builder(\n",
    "        dataset_path, name=dataset_name\n",
    "    ).info.splits.keys()\n",
    "    split = \"validation\" if \"validation\" in available_splits else \"test\"\n",
    "    split = split if SPLIT != \"train\" else \"train\"\n",
    "    print(f\"{split=}\")\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\n",
    "        dataset_path, name=dataset_name, split=split,\n",
    "    )\n",
    "\n",
    "    # Get a desired subset of the data\n",
    "    n = N_PER_DATASET if dataset.num_rows > N_PER_DATASET else dataset.num_rows\n",
    "    dataset = dataset.shuffle(seed=42).select(range(n))\n",
    "\n",
    "    print(dataset.num_rows)\n",
    "\n",
    "    key = f\"{dataset_path}/{dataset_name}\" if dataset_name else dataset_path\n",
    "    dataset_dict[key] = dataset\n",
    "\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([len(Counter(dataset[\"label\"])) > 1 for dataset in dataset_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_news: 7600\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(f\"{dataset_name}: {dataset.num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_news: 4\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(f\"{dataset_name}: {len(Counter(dataset['label']))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_news\n",
      "6600 1000\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "ppo_dataset_dict = {}\n",
    "test_dataset_dict = {}\n",
    "\n",
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(dataset_name)\n",
    "    splits = dataset.train_test_split(test_size=0.1315, seed=42)\n",
    "    # print(splits)\n",
    "    print(splits[\"train\"].num_rows, splits[\"test\"].num_rows)\n",
    "\n",
    "    ppo_dataset_dict[dataset_name] = splits[\"train\"]\n",
    "    test_dataset_dict[dataset_name] = splits[\"test\"]\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6600, 1000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dataset.num_rows for dataset in ppo_dataset_dict.values()), sum(dataset.num_rows for dataset in test_dataset_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    }
   ],
   "source": [
    "# datasets.DatasetDict(ppo_dataset_dict).save_to_disk(\"datasets/burns_datasets_VINC_ag_news_ppo_training_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    }
   ],
   "source": [
    "# datasets.DatasetDict(test_dataset_dict).save_to_disk(\"datasets/burns_datasets_VINC_ag_news_test_raw\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    ag_news: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set dataset_dict to test_dataset\n",
    "dataset_dict = datasets.DatasetDict.load_from_disk(\"datasets/burns_datasets_VINC_ag_news_test_raw\")\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dataset.num_rows for dataset in dataset_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_news: 4\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(f\"{dataset_name}: {len(Counter(dataset['label']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_template_dict = {}\n",
    "\n",
    "for dataset_path in dataset_dict.keys():\n",
    "    dataset_templates = DatasetTemplates(dataset_path)\n",
    "\n",
    "    dataset_templates.templates = {\n",
    "        x.name: x for x in dataset_templates.templates.values() if x.get_answer_choices_list(dataset_dict[dataset_path][0]) is not None\n",
    "    }\n",
    "\n",
    "    dataset_template_dict[dataset_path] = dataset_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_news: 15\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "    print(f\"{dataset_name}: {len(dataset_templates.templates)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"ag_news\"\n",
    "\n",
    "# for template_name, template in dataset_template_dict[dataset_name].templates.items():\n",
    "#     # print(template_name)\n",
    "#     # print(dataset_dict[dataset_name][0])\n",
    "#     q, a = template.apply(\n",
    "#         dataset_dict[dataset_name][0]\n",
    "#     )\n",
    "#     # print(q == q.strip())\n",
    "#     # print(a == a.strip())\n",
    "#     # print(\" \".join([q, a.strip()]))\n",
    "#     print(\" \".join([q, a]))\n",
    "#     print(\"---------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form the dataset for the chosen split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "#     print(dataset_name)\n",
    "#     for template_name, template in dataset_templates.templates.items():\n",
    "#         print(f\"{template_name}: {template.get_fixed_answer_choices_list()}\")\n",
    "\n",
    "#     print(\"---------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "#     print(dataset_name)\n",
    "#     for template_name, template in dataset_templates.templates.items():\n",
    "#         print(f\"{template_name}: {template.get_answer_choices_list(dataset_dict[dataset_name][0])}\")\n",
    "\n",
    "#     print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_news: 15\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "    print(f\"{dataset_name}: {len(dataset_templates.templates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_news\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.05 s, sys: 1.31 ms, total: 2.05 s\n",
      "Wall time: 2.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(2023)\n",
    "\n",
    "ALLOWED_KEYS = [\"text\", \"label\", \"original_dataset\", \"template_name\"]\n",
    "\n",
    "new_dataset = []\n",
    "\n",
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(dataset_name)\n",
    "\n",
    "    # if dataset_name != \"ag_news\": continue\n",
    "    # if dataset_name != \"imdb\": continue\n",
    "    # if dataset_name != \"piqa\": continue\n",
    "\n",
    "    for idx, entry in enumerate(dataset):\n",
    "        new_entry = entry.copy()\n",
    "        \n",
    "        # In case we need to know which dataset the entry came from\n",
    "        new_entry[\"original_dataset\"] = dataset_name\n",
    "\n",
    "        # Sample a random template\n",
    "        template_name = random.choice(\n",
    "            list(dataset_template_dict[dataset_name].templates.keys())\n",
    "        )\n",
    "        template = dataset_template_dict[dataset_name].templates[template_name]\n",
    "        new_entry[\"template_name\"] = template_name\n",
    "\n",
    "        # Whether the sample will be truthful or not\n",
    "        is_truthful = random.choice([True, False])\n",
    "\n",
    "        # If not truthful, sample a random incorrect label\n",
    "        if not is_truthful and len(dataset.features[\"label\"].names) > 2:\n",
    "            label_mappping = dataset.features[\"label\"] # [class 1, class 2, ...]\n",
    "            all_label_ids = [label_mappping.str2int(x) for x in label_mappping.names] # [0, 1, ...]\n",
    "            incorrect_label_id = all_label_ids[(1 - new_entry[\"label\"]) % len(all_label_ids)]\n",
    "            incorrect_label = template.get_fixed_answer_choices_list()[incorrect_label_id]\n",
    "            \n",
    "        \n",
    "        # Apply the template\n",
    "        if is_truthful:\n",
    "            new_text = \" \".join(template.apply(new_entry))\n",
    "        elif not is_truthful and len(dataset.features[\"label\"].names) > 2:\n",
    "            q, a = template.apply(new_entry)\n",
    "            incorrect_label = replace_text_with_whitespace(a, incorrect_label)\n",
    "            new_text = \" \".join([q, incorrect_label])\n",
    "        else:\n",
    "            # Untruthful binary case\n",
    "            new_entry[\"label\"] = 1 - new_entry[\"label\"]\n",
    "            new_text = \" \".join(template.apply(new_entry))\n",
    "\n",
    "        new_entry[\"text\"] = new_text\n",
    "\n",
    "        # We can now change the label to whether the sample is truthful or not\n",
    "        new_entry[\"label\"] = int(is_truthful)\n",
    "\n",
    "        # Remove all other keys\n",
    "        new_entry = {k: v for k, v in new_entry.items() if k in ALLOWED_KEYS}\n",
    "\n",
    "        # Append to the new dataset\n",
    "        new_dataset.append(new_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'original_dataset', 'template_name'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset = datasets.Dataset.from_list(new_dataset)\n",
    "my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Consider the following example: ''' IMF sees rising oil prices having little impact on global growth TOKYO: Rising oil prices are unlikely to deal a major blow to global economic growth although the trend may seem  quot;uncomfortable, quot; a researcher with the International Monetary Fund says. ''' Choice 1: Business. Choice 2: Science and technology.Between choice 1 and choice 2, what is the topic of this example? Science and technology\",\n",
       " 'label': 0,\n",
       " 'original_dataset': 'ag_news',\n",
       " 'template_name': 'burns_2'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({1: 512, 0: 488}), Counter({'ag_news': 1000}))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(my_dataset[\"label\"]), Counter(my_dataset[\"original_dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 397.94ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "390186"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my_dataset.to_parquet(f\"datasets/burns_datasets_VINC_ag_news_validation.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 312K\n",
      "drwxr-xr-x 4 augustas Domain Users  33K Jun 19 16:07 .\n",
      "drwxr-xr-x 4 augustas Domain Users  33K Jun 19 15:59 ..\n",
      "drwxr-xr-x 3 augustas Domain Users  25K Jun 19 16:07 burns_datasets_VINC_ag_news_ppo_training_raw\n",
      "drwxr-xr-x 3 augustas Domain Users  25K Jun 19 15:59 burns_datasets_VINC_ag_news_test_raw\n",
      "-rw-r--r-- 1 augustas Domain Users 188K Jun 19 16:00 burns_datasets_VINC_ag_news_train.parquet\n",
      "-rw-r--r-- 1 augustas Domain Users 187K Jun 19 16:12 burns_datasets_VINC_ag_news_validation.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lah datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
