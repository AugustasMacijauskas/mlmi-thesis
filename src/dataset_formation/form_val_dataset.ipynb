{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset_builder, load_dataset\n",
    "\n",
    "from utils import replace_text_with_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/fsx/home-augustas/elk')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ELK_PATH = Path(\"../../../elk/\")\n",
    "ELK_PATH.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/fsx/home-augustas/elk/elk/promptsource', '/fsx/home-augustas/elk']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules = [\n",
    "    ELK_PATH,\n",
    "    ELK_PATH / \"elk\" / \"promptsource\",\n",
    "]\n",
    "\n",
    "for module in modules:\n",
    "    if not str(module) in sys.path:\n",
    "        sys.path.insert(0, str(module.resolve()))\n",
    "\n",
    "sys.path[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from templates import DatasetTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable the logging of the datasets library\n",
    "import datasets\n",
    "\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all datasets used in Burns et al. (2022) (apart from story_cloze)\n",
    "# BURNS_DATASETS = [\n",
    "#     \"ag_news\",\n",
    "#     \"amazon_polarity\",\n",
    "#     \"dbpedia_14\",\n",
    "#     \"glue:qnli\",\n",
    "#     \"imdb\",\n",
    "#     \"piqa\",\n",
    "#     \"super_glue:boolq\",\n",
    "#     \"super_glue:copa\",\n",
    "#     \"super_glue:rte\",\n",
    "# ]\n",
    "# BURNS_DATASETS = [\n",
    "#     \"ag_news\",\n",
    "#     \"amazon_polarity\",\n",
    "#     \"dbpedia_14\",\n",
    "#     \"glue:qnli\",\n",
    "#     \"imdb\",\n",
    "#     \"super_glue:boolq\",\n",
    "#     \"super_glue:copa\",\n",
    "#     \"super_glue:rte\",\n",
    "# ]\n",
    "# BURNS_DATASETS = [\n",
    "#     \"ag_news\",\n",
    "#     \"amazon_polarity\",\n",
    "#     \"dbpedia_14\",\n",
    "#     \"imdb\",\n",
    "#     \"super_glue:boolq\",\n",
    "#     \"super_glue:copa\",\n",
    "# ]\n",
    "BURNS_DATASETS = [\"imdb\"]\n",
    "\n",
    "version_number = 5\n",
    "VERSION = f\"v1{version_number}\"\n",
    "\n",
    "# SPLIT = \"train\"\n",
    "SPLIT = \"validation\"\n",
    "\n",
    "# These numbers are chosen so that both datasets have\n",
    "# approximately 10k examples in total (probably a bit less for train split)\n",
    "# N_PER_DATASET = 1000 if SPLIT == \"train\" else 1000\n",
    "# N_PER_DATASET = 1200 if SPLIT == \"train\" else 1375\n",
    "# N_PER_DATASET = 1200 if SPLIT == \"train\" else 1550\n",
    "N_PER_DATASET = 25000 if SPLIT == \"train\" else 1500\n",
    "if SPLIT == \"train\":\n",
    "    # assert N_PER_DATASET <= 2490, \"N_PER_DATASET must be <= 2490\"\n",
    "    assert N_PER_DATASET <= 25000, \"N_PER_DATASET must be <= 2490\"\n",
    "else:\n",
    "    assert N_PER_DATASET <= 1838, \"N_PER_DATASET must be <= 1838\"\n",
    "\n",
    "# SEED = 42 if SPLIT == \"train\" else 2023\n",
    "SEED = 42 if SPLIT == \"train\" else version_number\n",
    "SEED"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = {}\n",
    "for dataset_path in BURNS_DATASETS:\n",
    "    print(dataset_path)\n",
    "\n",
    "    # Parse dataset name\n",
    "    dataset_name = None    \n",
    "    if \":\" in dataset_path:\n",
    "        dataset_path, dataset_name = dataset_path.split(\":\")\n",
    "    \n",
    "    \n",
    "    # Get the most validation-like split\n",
    "    available_splits = load_dataset_builder(\n",
    "        dataset_path, name=dataset_name\n",
    "    ).info.splits.keys()\n",
    "    split = \"validation\" if \"validation\" in available_splits else \"test\"\n",
    "    split = split if SPLIT != \"train\" else \"train\"\n",
    "    print(split)\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\n",
    "        dataset_path, name=dataset_name, split=split,\n",
    "    )\n",
    "\n",
    "    # Get a desired subset of the data\n",
    "    n = N_PER_DATASET if dataset.num_rows > N_PER_DATASET else dataset.num_rows\n",
    "    dataset = dataset.shuffle(seed=SEED).select(range(n))\n",
    "\n",
    "    key = f\"{dataset_path}/{dataset_name}\" if dataset_name else dataset_path\n",
    "    dataset_dict[key] = dataset\n",
    "\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dataset.num_rows for dataset in dataset_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([len(Counter(dataset[\"label\"])) > 1 for dataset in dataset_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb: 1500\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(f\"{dataset_name}: {dataset.num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb: 2\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(f\"{dataset_name}: {len(Counter(dataset['label']))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_template_dict = {}\n",
    "\n",
    "for dataset_path in dataset_dict.keys():\n",
    "    dataset_templates = DatasetTemplates(dataset_path)\n",
    "\n",
    "    dataset_templates.templates = {\n",
    "        x.name: x for x in dataset_templates.templates.values() if x.get_answer_choices_list(dataset_dict[dataset_path][0]) is not None\n",
    "    }\n",
    "\n",
    "    dataset_template_dict[dataset_path] = dataset_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb: 13\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "    print(f\"{dataset_name}: {len(dataset_templates.templates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"ag_news\"\n",
    "# for idx, (template_name, template) in enumerate(dataset_template_dict[dataset_name].templates.items()):\n",
    "#     print(f\"{idx+1}) {dataset_name}/{template_name}: {template.metadata.choices_in_prompt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "#     good_templates = [x for x in dataset_templates.templates.values() if x.metadata.choices_in_prompt]\n",
    "#     print(f\"{dataset_name}: {len(good_templates)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"super_glue/rte\"\n",
    "# dataset_name = \"glue/qnli\"\n",
    "\n",
    "# for template_name, template in dataset_template_dict[dataset_name].templates.items():\n",
    "#     # print(template_name)\n",
    "#     # print(dataset_dict[dataset_name][0])\n",
    "#     q, a = template.apply(\n",
    "#         dataset_dict[dataset_name][1]\n",
    "#     )\n",
    "#     # print(q == q.strip())\n",
    "#     # print(a == a.strip())\n",
    "#     # print(\" \".join([q, a.strip()]))\n",
    "#     print(\" \".join([q, a]))\n",
    "#     print(len(a))\n",
    "#     print(\"---------------------------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form the dataset for the chosen split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "#     print(dataset_name)\n",
    "#     for template_name, template in dataset_templates.templates.items():\n",
    "#         print(f\"{template_name}: {template.get_fixed_answer_choices_list()}\")\n",
    "\n",
    "#     print(\"---------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "#     print(dataset_name)\n",
    "#     for template_name, template in dataset_templates.templates.items():\n",
    "#         print(f\"{template_name}: {template.get_answer_choices_list(dataset_dict[dataset_name][0])}\")\n",
    "\n",
    "#     print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb: 13\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset_templates in dataset_template_dict.items():\n",
    "    print(f\"{dataset_name}: {len(dataset_templates.templates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb\n",
      "CPU times: user 2.3 s, sys: 16.6 ms, total: 2.32 s\n",
      "Wall time: 2.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(SEED)\n",
    "\n",
    "ALLOWED_KEYS = [\"text\", \"label\", \"original_dataset\", \"template_name\"]\n",
    "\n",
    "new_dataset = []\n",
    "\n",
    "for dataset_name, dataset in dataset_dict.items():\n",
    "    print(dataset_name)\n",
    "\n",
    "    for idx, entry in enumerate(dataset):\n",
    "        new_entry = entry.copy()\n",
    "        \n",
    "        # In case we need to know which dataset the entry came from\n",
    "        new_entry[\"original_dataset\"] = dataset_name\n",
    "\n",
    "        # Sample a random template\n",
    "        template_name = random.choice(\n",
    "            list(dataset_template_dict[dataset_name].templates.keys())\n",
    "        )\n",
    "        template = dataset_template_dict[dataset_name].templates[template_name]\n",
    "        new_entry[\"template_name\"] = template_name\n",
    "\n",
    "        # Whether the sample will be truthful or not\n",
    "        is_truthful = random.choice([True, False])\n",
    "\n",
    "        # If not truthful, sample a random incorrect label\n",
    "        if not is_truthful and len(dataset.features[\"label\"].names) > 2:\n",
    "            label_mappping = dataset.features[\"label\"] # [class 1, class 2, ...]\n",
    "            all_label_ids = [label_mappping.str2int(x) for x in label_mappping.names] # [0, 1, ...]\n",
    "            incorrect_label_id = all_label_ids[(1 - new_entry[\"label\"]) % len(all_label_ids)]\n",
    "            incorrect_label = template.get_fixed_answer_choices_list()[incorrect_label_id]\n",
    "            \n",
    "        \n",
    "        # Apply the template\n",
    "        if is_truthful:\n",
    "            new_text = \" \".join(template.apply(new_entry))\n",
    "        elif not is_truthful and len(dataset.features[\"label\"].names) > 2:\n",
    "            q, a = template.apply(new_entry)\n",
    "            incorrect_label = replace_text_with_whitespace(a, incorrect_label)\n",
    "            new_text = \" \".join([q, incorrect_label])\n",
    "        else:\n",
    "            # Untruthful binary case\n",
    "            new_entry[\"label\"] = 1 - new_entry[\"label\"]\n",
    "            new_text = \" \".join(template.apply(new_entry))\n",
    "\n",
    "        new_entry[\"text\"] = new_text\n",
    "\n",
    "        # We can now change the label to whether the sample is truthful or not\n",
    "        new_entry[\"label\"] = int(is_truthful)\n",
    "\n",
    "        # Remove all other keys\n",
    "        new_entry = { k: v for k, v in new_entry.items() if k in ALLOWED_KEYS }\n",
    "\n",
    "        # Append to the new dataset\n",
    "        new_dataset.append(new_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'original_dataset', 'template_name'],\n",
       "    num_rows: 1500\n",
       "})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset = datasets.Dataset.from_list(new_dataset)\n",
    "my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'At the very beginning, the look at a control panel that reads \"8 miles of the cost of California\", and no, I didn\\'t misspell that, they really did not realize the put of the cost instead of off the coast. These people must have been morons.<br /><br />It\\'s good if you\\'re into terrible movies, but the sheer fact they couldn\\'t catch a simple spelling issue make me believe they really didn\\'t put any effort into creating the movie whatsoever. The Navy uniforms are not correct at all in any manner whatsoever.<br /><br />Wow, completely ridiculous, but good if you are looking for something insanely stupid to watch. How these folks made any money off this is beyond me.\\nWhat is the sentiment expressed by the reviewer for the movie?\\n positive',\n",
       " 'label': 0,\n",
       " 'original_dataset': 'imdb',\n",
       " 'template_name': 'Reviewer Expressed Sentiment'}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_idx = 0\n",
    "my_dataset[current_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At the very beginning, the look at a control panel that reads \"8 miles of the cost of California\", and no, I didn't misspell that, they really did not realize the put of the cost instead of off the coast. These people must have been morons.<br /><br />It's good if you're into terrible movies, but the sheer fact they couldn't catch a simple spelling issue make me believe they really didn't put any effort into creating the movie whatsoever. The Navy uniforms are not correct at all in any manner whatsoever.<br /><br />Wow, completely ridiculous, but good if you are looking for something insanely stupid to watch. How these folks made any money off this is beyond me.\n",
      "What is the sentiment expressed by the reviewer for the movie?\n",
      " positive\n"
     ]
    }
   ],
   "source": [
    "# It's unbelievable but the fourth is better\n",
    "# The following movie review expresses what\n",
    "# I was going to use 'The German Scream' as a summary\n",
    "# This movie is by far one of the worst B-movies\n",
    "# At the very beginning, the look at a control\n",
    "print(my_dataset[current_idx][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0: 765, 1: 735}), Counter({'imdb': 1500}))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counter({0: 755, 1: 745})\n",
    "# Counter({1: 750, 0: 750})\n",
    "# Counter({0: 774, 1: 726}\n",
    "# Counter({0: 752, 1: 748})\n",
    "# Counter({0: 765, 1: 735})\n",
    "Counter(my_dataset[\"label\"]), Counter(my_dataset[\"original_dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6741abf2b2a744f7bde552bf97068e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1881013"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my_dataset.to_parquet(f\"datasets/burns_datasets_VINC_{SPLIT}_{VERSION}.parquet\")\n",
    "# my_dataset.to_parquet(f\"datasets/burns_datasets_VINC_imdb_{SPLIT}_{VERSION}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x  3 augustas Domain Users  25K Jul  4 22:35 burns_datasets_VINC_imdb_train_raw_v15\n",
      "-rw-r--r--  1 augustas Domain Users 7.4M Jul  4 22:35 burns_datasets_VINC_imdb_train_v15.parquet\n",
      "-rw-r--r--  1 augustas Domain Users 1.2M Jul  4 22:44 burns_datasets_VINC_imdb_validation_v15.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lah datasets | grep {VERSION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet_data_files = {\n",
    "#     \"train\": \"datasets/my_dataset_train.parquet\",\n",
    "#     \"validation\": \"datasets/my_dataset_validation.parquet\",\n",
    "# }\n",
    "\n",
    "# my_dataset = load_dataset(\"parquet\", data_files=parquet_data_files)\n",
    "# my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
