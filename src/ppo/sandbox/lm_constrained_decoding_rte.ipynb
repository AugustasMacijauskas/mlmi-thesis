{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "import datasets\n",
    "# Don't show progress datasets bars\n",
    "datasets.disable_progress_bar()\n",
    "from datasets import load_dataset\n",
    "\n",
    "from fastchat.model import get_conversation_template\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd().parent.resolve()))\n",
    "from model import get_model\n",
    "from dataset import collator\n",
    "from utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    f\"cuda:{torch.cuda.current_device()}\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"lmsys/vicuna-7b-v1.3\"\n",
    "\n",
    "dataset_path = \"super_glue\"\n",
    "dataset_name = \"rte\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer lmsys/vicuna-7b-v1.3...\n",
      "Loaded tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = get_tokenizer(tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset super_glue (/admin/home-augustas/.cache/huggingface/datasets/super_glue/rte/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'idx', 'label'],\n",
      "    num_rows: 2490\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Dataset for PPO training\n",
    "dataset = load_dataset(dataset_path, dataset_name, split=\"train\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Consider the premise below in triple backticks and the corresponding statement. Based on the information in the premise, tell whether the statement is correct. Your answer should be either true or false.\n",
      "\n",
      "Desired format:\n",
      "Answer: <your_answer>\n",
      "Do not print \"Answer:\" again, just what you think the answer is.\n",
      "\n",
      "Premise:\n",
      "```\n",
      "Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients.\n",
      "```\n",
      "Statement: Herceptin can be used to treat breast cancer.\n",
      "Answer: ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "# def doc_to_text(doc):\n",
    "#     return \"{}\\nQuestion: {} True or False?\\nAnswer:\".format(\n",
    "#         doc[\"premise\"],\n",
    "#         doc[\"hypothesis\"],\n",
    "#     )\n",
    "\n",
    "\n",
    "def format_label(label):\n",
    "    return \"False\" if label else \"True\"\n",
    "\n",
    "# Dataset template\n",
    "def create_get_prompt_fn(tokenizer_name):\n",
    "    def get_prompt(example, answer_prefix=\"Answer:\"):\n",
    "        conv = get_conversation_template(tokenizer_name)\n",
    "\n",
    "        message = (\n",
    "            \"Consider the premise below in triple backticks \"\n",
    "            \"and the corresponding statement. Based on the information in the premise, \"\n",
    "            \"tell whether the statement is correct. Your answer should be either true or false.\\n\\n\"\n",
    "            \"Desired format:\\n\"\n",
    "            \"Answer: <your_answer>\\n\"\n",
    "            f\"Do not print \\\"{answer_prefix}\\\" again, just what you think the answer is.\\n\\n\"\n",
    "            f\"Premise:\\n```\\n{example['premise']}\\n```\\n\"\n",
    "            f\"Statement: {example['hypothesis']}\\n\"\n",
    "            f\"{answer_prefix}\"\n",
    "        )\n",
    "\n",
    "        conv.append_message(conv.roles[0], message)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "\n",
    "        return conv.get_prompt()\n",
    "\n",
    "    return get_prompt\n",
    "\n",
    "get_prompt = create_get_prompt_fn(tokenizer_name)\n",
    "input_text = get_prompt(dataset[2])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading policy model...\n",
      "\n",
      "is_bf16_possible=False\n",
      "kwargs={'load_in_8bit': True, 'low_cpu_mem_usage': True, 'torch_dtype': None}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8c952d213e423f9f53d9b3d0c50568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded subject model with 6,738,419,713 parameters.\n",
      "Model dtype: torch.float16\n",
      "\n",
      "memory_usage=6.58 GB\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "# model = get_model(tokenizer_name, device)\n",
    "model = get_model(tokenizer_name, device, load_in_8bit=True, low_cpu_mem_usage=True)\n",
    "\n",
    "memory_usage = model.pretrained_model.get_memory_footprint() / (1024 ** 3)\n",
    "print(f\"{memory_usage=:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'hypothesis', 'idx', 'label', 'prompt', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2490\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_prompt(batch):\n",
    "    processed_batch = defaultdict(list)\n",
    "    for premise, hypothesis in zip(batch[\"premise\"], batch[\"hypothesis\"]):\n",
    "        example = { \"premise\": premise, \"hypothesis\": hypothesis }\n",
    "        processed_batch[\"prompt\"].append(get_prompt(example))\n",
    "    \n",
    "    return processed_batch\n",
    "\n",
    "def filter_too_long(batch):\n",
    "    # Substract a bit more to allow for generations to be processed\n",
    "    return [len(x) < tokenizer.model_max_length - 8 for x in batch[\"input_ids\"]]\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "    lambda batch: apply_prompt(batch), batched=True, num_proc=12\n",
    ")\n",
    "processed_dataset = processed_dataset.map(\n",
    "    lambda batch: tokenizer(batch[\"prompt\"]), batched=True, num_proc=12\n",
    ")\n",
    "processed_dataset = processed_dataset.filter(filter_too_long, batched=True, num_proc=12)\n",
    "\n",
    "# Remove some columns\n",
    "processed_dataset = processed_dataset.remove_columns([\"token_type_ids\"])\n",
    "\n",
    "processed_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], output_all_columns=True)\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the processed dataset to hub\n",
    "# processed_dataset.push_to_hub(\"AugustasM/imdb_vicuna\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer, PPOConfig\n",
    "\n",
    "config = PPOConfig()\n",
    "\n",
    "optimizer = None\n",
    "\n",
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config,\n",
    "    model,\n",
    "    ref_model=None,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    data_collator=collator,\n",
    "    optimizer=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader length: 39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b780cedd6347909f246cccd0fd60d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2490, 2490)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    processed_dataset,\n",
    "    # processed_dataset.shuffle(seed=42).select(range(8)),\n",
    "    # processed_dataset.shuffle(seed=42).select(range(1024)),\n",
    "    batch_size=64, collate_fn=collator,\n",
    "    num_workers=12, shuffle=False,\n",
    ")\n",
    "print(f\"Dataloader length: {len(dataloader)}\")\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": False,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": 100_000, # why is this value like this?\n",
    "    \"pad_to_multiple_of\": 8, # TODO: double-check, but this seems to work and to be faster\n",
    "    \"max_new_tokens\": 1,\n",
    "}\n",
    "\n",
    "gold_outputs = []\n",
    "outputs = []\n",
    "for batch in tqdm(dataloader, total=len(dataloader), leave=False):\n",
    "    batch_gold_outputs = [format_label(label) for label in batch[\"label\"]]\n",
    "    gold_outputs.extend(batch_gold_outputs)\n",
    "\n",
    "    question_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    response_tensors = ppo_trainer.generate(\n",
    "        question_tensors,\n",
    "        return_prompt=False,\n",
    "        # length_sampler=output_length_sampler, # TODO: can be none\n",
    "        batch_size=8, # TODO: generations are made in batches\n",
    "        **generation_kwargs,\n",
    "    )\n",
    "    batch[\"response\"] = tokenizer.batch_decode(\n",
    "        response_tensors, skip_special_tokens=True, spaces_between_special_tokens=False\n",
    "    )\n",
    "\n",
    "    outputs.extend(batch[\"response\"])\n",
    "\n",
    "len(outputs), len(gold_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for output, golden_output in zip(outputs, gold_outputs):\n",
    "#     print(f\"{golden_output=}\\n{output=}\")\n",
    "#     print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2490 2490\n",
      "Counter({'False': 1380, 'True': 1109, 'strik': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(len(gold_outputs), len(outputs))\n",
    "print(Counter(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7514056224899598"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_accuracy(gold_outputs, outputs):\n",
    "    return sum([1 if gold == output else 0 for gold, output in zip(gold_outputs, outputs)]) / len(outputs)\n",
    "\n",
    "get_accuracy(gold_outputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# for output, golden_output in zip(outputs, gold_outputs):\n",
    "#     if output != golden_output:\n",
    "#         print(f\"{golden_output=}\\n{output=}\")\n",
    "#         print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
