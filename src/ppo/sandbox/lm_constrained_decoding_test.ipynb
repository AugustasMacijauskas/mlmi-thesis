{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "import datasets\n",
    "# Don't show progress datasets bars\n",
    "datasets.disable_progress_bar()\n",
    "from datasets import load_dataset\n",
    "\n",
    "from fastchat.model import get_conversation_template\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd().parent.resolve()))\n",
    "from model import get_model\n",
    "from utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    f\"cuda:{torch.cuda.current_device()}\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_name = \"gpt2-xl\"\n",
    "# tokenizer_name = \"databricks/dolly-v2-3b\"\n",
    "# tokenizer_name = \"meta-llama/Llama-2-13b-hf\"\n",
    "tokenizer_name = \"lmsys/vicuna-7b-v1.3\"\n",
    "\n",
    "# dataset_name = \"AugustasM/burns-datasets-VINC-imdb-ppo-training-v2\"\n",
    "dataset_name = \"imdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer lmsys/vicuna-7b-v1.3...\n",
      "Loaded tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = get_tokenizer(tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/admin/home-augustas/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Loading cached shuffled indices for dataset at /admin/home-augustas/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-9c48ce5d173413c7.arrow\n",
      "Loading cached shuffled indices for dataset at /admin/home-augustas/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-9c48ce5d173413c7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(few_shot_examples_dataset)=5\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 24995\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Dataset for PPO training\n",
    "tmp_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "few_shot_examples_dataset = tmp_dataset.shuffle(seed=42).select(range(5)).to_list()\n",
    "dataset = tmp_dataset.shuffle(seed=42).select(range(5, len(tmp_dataset)))\n",
    "\n",
    "print(f\"{len(few_shot_examples_dataset)=}\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the movie review as either positive or negative.\n",
      "\n",
      "Desired format:\n",
      "Sentiment: <identified_sentiment>\n",
      "Do not print \"Sentiment:\" again, just the sentiment.\n",
      "\n",
      "Movie review: ```\n",
      "While this movie's style isn't as understated and realistic as a sound version probably would have been, this is still a very good film. In fact, it was seen as an excellent film in its day, as it was nominated for the first Best Picture Oscar (losing to WINGS). I still consider WINGS to be a superior film, but this one is excellent despite a little bit of overacting by the lead, Emil Jannings.<br /><br />Jannings is a general from Czarist Russia who is living out his final days making a few bucks in the 1920s by being a Hollywood extra. His luck appears to have changed as he gets a casting call--to play an Imperial Russian general fighting against the Communists during the revolution. Naturally this isn't much of a stretch acting-wise, but it also gets the old man to thinking about the old days and the revolution.<br /><br />Exactly what happens next I'll leave to you, but it's a pretty good film--particularly at the end. By the way, look for William Powell as the Russian director. Despite being made in 1928, with the makeup he doesn't look much younger than he did in many of his later films.\n",
      "```\n",
      "Sentiment:\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(label):\n",
    "    # return \"Positive\" if label else \"Negative\"\n",
    "    return \"positive\" if label else \"negative\"\n",
    "\n",
    "# Dataset template\n",
    "def get_prompt_template(few_shot_examples_dataset):\n",
    "    def prompt_template(example, num_few_shot_examples=0):\n",
    "        few_shot_examples = \"\"\n",
    "        for i in range(num_few_shot_examples):\n",
    "            item = few_shot_examples_dataset[i]\n",
    "\n",
    "            sentiment = get_sentiment(item[\"label\"])\n",
    "            few_shot_examples += (\n",
    "                f\"Movie review: ```\\n{item['text']}```\\n\"\n",
    "                f\"Sentiment: {sentiment}\\n\"\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            \"Classify the movie review as either positive or negative.\\n\\n\"\n",
    "            f\"{few_shot_examples}\"\n",
    "            \"Desired format:\\n\"\n",
    "            \"Sentiment: <identified_sentiment>\\n\"\n",
    "            \"Do not print \\\"Sentiment:\\\" again, just the sentiment.\\n\\n\"\n",
    "            f\"Movie review: ```\\n{example['text']}\\n```\\n\"\n",
    "            f\"Sentiment:\"\n",
    "        )\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "apply_template = get_prompt_template(few_shot_examples_dataset)\n",
    "input_text = apply_template(dataset[0], num_few_shot_examples=0)\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading policy model...\n",
      "\n",
      "is_bf16_possible=False\n",
      "kwargs={'load_in_8bit': True, 'torch_dtype': None}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e181f765a7e9485f952485d9ca56f5f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded subject model with 6,738,419,713 parameters.\n",
      "Model dtype: torch.float16\n",
      "\n",
      "memory_usage=6.58 GB\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "# model = get_model(tokenizer_name, device)\n",
    "model = get_model(tokenizer_name, device, load_in_8bit=True)\n",
    "\n",
    "memory_usage = model.pretrained_model.get_memory_footprint() / (1024 ** 3)\n",
    "print(f\"{memory_usage=:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784d1dabc55147b7948d01bed1f379fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label='Positive'\n",
      "outputs='Positive'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "label='Positive'\n",
      "outputs='Positive'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "label='Negative'\n",
      "outputs='Negative'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "label='Negative'\n",
      "outputs='Negative'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "label='Positive'\n",
      "outputs='Positive'\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TODO: do I need to set the pad_token?\n",
    "generation_kwargs = {\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 1.0,\n",
    "    # \"do_sample\": True,\n",
    "    \"do_sample\": False,\n",
    "    # \"pad_to_multiple_of\": 8,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": 100_000, # why is this value like this?\n",
    "    \"max_new_tokens\": 4,\n",
    "    # \"repetition_penalty\": 1.2,\n",
    "}\n",
    "\n",
    "for i, example in tqdm(enumerate(dataset), total=len(dataset), leave=False):\n",
    "    label = \"Positive\" if example[\"label\"] else \"Negative\"\n",
    "    print(f\"{label=}\")\n",
    "    label_ids = tokenizer([label])[\"input_ids\"][0]\n",
    "    generation_kwargs[\"max_new_tokens\"] = max(len(label_ids) + 1, 4)\n",
    "\n",
    "    input_text = apply_template(example, num_few_shot_examples=0)\n",
    "    \n",
    "    conv = get_conversation_template(tokenizer_name)\n",
    "    conv.append_message(conv.roles[0], input_text)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "    inputs.pop(\"token_type_ids\")\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        **generation_kwargs,\n",
    "    )\n",
    "    output_ids = output_ids[0][len(inputs[\"input_ids\"][0]):]\n",
    "    outputs = tokenizer.decode(\n",
    "        output_ids, skip_special_tokens=True, spaces_between_special_tokens=False\n",
    "    )\n",
    "    # outputs = outputs.strip().lower()\n",
    "    # outputs = outputs.lower()\n",
    "    print(f\"{outputs=}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    if i >= 4: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Classify the movie review as either positive or negative.\n",
      "\n",
      "Desired format:\n",
      "Sentiment: <identified_sentiment>\n",
      "Do not print \"Sentiment:\" again, just the sentiment.\n",
      "\n",
      "Movie review: ```\n",
      "Home Room deals with a Columbine-like high-school shooting but rather than hashing over the occurrence itself the film portrays the aftermath and what happened to the survivors, their trauma, guilt and denial.<br /><br />*Spoilers* The shooting itself is treated as a foregone conclusion, with no action footage other than the reaction of an almost teenage SWAT commando after shooting the high school killer. The film has three protagonists; the detective investigating the crime of which no guilty parties are left to convict and two teenage girls surviving the incident, played by a very young Erika Christensen and Busy Philipps.<br /><br />The two girls having nothing in common besides the shooting are put together because of it and the drama ensues.<br /><br />Erika Christensen, though only 24 has been around the block so much that film viewers are pretty much acquainted with her solid and reliable style of acting. Busy Philipps, three years older than Christensen and altogether unknown to me, blew me away with her overwhelming dramatic strength and screen presence. This girl was the part.<br /><br />It's a great movie and it connects to you with its intimate focus on the fragile yet growing relationship between the two traumatized girls. Gus van Sant's Elephant (2003) though good, seems almost superficial and paltry compared to Home Room when it comes to dramatic flair and acting. What I can see this film got very little screen time and exposure - so much more a loss for an equally traumatized America.<br /><br />Ten out of Ten\n",
      "```\n",
      "Sentiment:\n",
      "ASSISTANT: Positive\n"
     ]
    }
   ],
   "source": [
    "print(f\"{conv.roles[0]}: {input_text}\")\n",
    "print(f\"{conv.roles[1]}: {outputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.pretrained_model.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262410240"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_trainable_params = sum(p.numel() for p in model.pretrained_model.parameters() if p.requires_grad)\n",
    "num_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Tuple, Union\n",
    "\n",
    "\n",
    "def rhasattr(obj, attr):\n",
    "    \"\"\"A chain-able attribute version of hasattr. For example, to check if\n",
    "    `obj` has the attribute `foo.bar.baz`, you can use:\n",
    "        `rhasattr(obj, \"foo.bar.baz\")`\n",
    "    Reference: https://stackoverflow.com/a/67303315\n",
    "    \"\"\"\n",
    "    _nested_attrs = attr.split(\".\")\n",
    "    _curr_obj = obj\n",
    "    for _a in _nested_attrs[:-1]:\n",
    "        if hasattr(_curr_obj, _a):\n",
    "            _curr_obj = getattr(_curr_obj, _a)\n",
    "        else:\n",
    "            return False\n",
    "    return hasattr(_curr_obj, _nested_attrs[-1])\n",
    "\n",
    "\n",
    "def rgetattr(obj, attr: str, *args) -> object:\n",
    "    \"\"\"A chain-able attribute version of getattr. For example, to get the\n",
    "    attribute `foo.bar.baz` from `obj`, you can use:\n",
    "        `rgetattr(obj, \"foo.bar.baz\")`\n",
    "    Reference: https://stackoverflow.com/a/31174427\n",
    "    \"\"\"\n",
    "\n",
    "    def _getattr(obj, attr):\n",
    "        return getattr(obj, attr, *args)\n",
    "\n",
    "    return functools.reduce(_getattr, [obj] + attr.split(\".\"))\n",
    "\n",
    "def findattr(obj, attrs: Tuple[str]) -> Union[object, None]:\n",
    "    for attr in attrs:\n",
    "        if rhasattr(obj, attr):\n",
    "            return rgetattr(obj, attr)\n",
    "    raise ValueError(f\"Could not find an attribute from `{attrs}` in `{obj}`\")\n",
    "\n",
    "def hf_get_decoder_blocks(model: torch.nn.Module) -> Tuple[torch.nn.Module]:\n",
    "    \"\"\"Returns the decoder hidden layers of the specified model.\n",
    "    NOTE: Different model configurations have different hidden layer attribute names.\n",
    "        - transformer.h: (BloomForCausalLM, GPT2LMHeadModel, GPTJForCausalLM)\n",
    "        - model.decoder.layers: (OPTForCausalLM)\n",
    "        - gpt_neox.layers: (GPTNeoXForCausalLM)\n",
    "        - decoder.block: (T5ForConditionalGeneration)\n",
    "    \"\"\"\n",
    "    hidden_layers_attrs = (\n",
    "        \"h\",\n",
    "        \"layers\",\n",
    "        \"model.layers\",\n",
    "        \"decoder.layers\",\n",
    "        \"transformer.h\",\n",
    "        \"model.decoder.layers\",\n",
    "        \"gpt_neox.layers\",\n",
    "        \"decoder.block\",\n",
    "    )\n",
    "    return findattr(model, hidden_layers_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_bottom_causal_layers(model: torch.nn.Module, num_layers_unfrozen: int = 0):\n",
    "    \"\"\"Freezes the bottom transformer block layers of the specified model.\"\"\"\n",
    "    hidden_layers = hf_get_decoder_blocks(model)\n",
    "    if num_layers_unfrozen == 0:\n",
    "        hidden_layers_to_freeze = list(hidden_layers)\n",
    "    elif num_layers_unfrozen > 0:\n",
    "        hidden_layers_to_freeze = list(hidden_layers)[:-num_layers_unfrozen]\n",
    "    else:\n",
    "        hidden_layers_to_freeze = []\n",
    "    for layer in hidden_layers_to_freeze:\n",
    "        layer.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 model.pretrained_model.requires_grad_(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>freeze_bottom_causal_layers(model.pretrained_model, num_layers_unfrozen=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>)                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/fsx/home-augustas/anaconda3/envs/elk/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">30</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">requires_grad_</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2327 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">Module: self</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2328 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2329 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> p <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.parameters():                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2330 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>p.requires_grad_(requires_grad)                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2331 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2332 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2333 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">zero_grad</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, set_to_none: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">bool</span> = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>) -&gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>only Tensors of floating point dtype can require gradients\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 model.pretrained_model.requires_grad_(\u001b[94mTrue\u001b[0m)                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0mfreeze_bottom_causal_layers(model.pretrained_model, num_layers_unfrozen=\u001b[94m3\u001b[0m)                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/fsx/home-augustas/anaconda3/envs/elk/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m23\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m30\u001b[0m in \u001b[92mrequires_grad_\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2327 \u001b[0m\u001b[2;33m│   │   │   \u001b[0m\u001b[33mModule: self\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2328 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2329 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m p \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.parameters():                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2330 \u001b[2m│   │   │   \u001b[0mp.requires_grad_(requires_grad)                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2331 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2332 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2333 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mzero_grad\u001b[0m(\u001b[96mself\u001b[0m, set_to_none: \u001b[96mbool\u001b[0m = \u001b[94mTrue\u001b[0m) -> \u001b[94mNone\u001b[0m:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0monly Tensors of floating point dtype can require gradients\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model.pretrained_model.requires_grad_(True)\n",
    "\n",
    "# freeze_bottom_causal_layers(model.pretrained_model, num_layers_unfrozen=3)\n",
    "\n",
    "# num_trainable_params = sum(p.numel() for p in model.pretrained_model.parameters() if p.requires_grad)\n",
    "# num_trainable_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
