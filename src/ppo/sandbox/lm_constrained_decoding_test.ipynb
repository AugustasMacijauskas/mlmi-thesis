{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "import datasets\n",
    "# Don't show progress datasets bars\n",
    "datasets.disable_progress_bar()\n",
    "from datasets import load_dataset\n",
    "\n",
    "from fastchat.model import get_conversation_template\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd().parent.resolve()))\n",
    "from model import get_model\n",
    "from dataset import collator\n",
    "from utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    f\"cuda:{torch.cuda.current_device()}\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"lmsys/vicuna-7b-v1.3\"\n",
    "\n",
    "dataset_name = \"imdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer lmsys/vicuna-7b-v1.3...\n",
      "Loaded tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = get_tokenizer(tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/admin/home-augustas/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Dataset for PPO training\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Classify the movie review as either positive or negative.\n",
      "\n",
      "Desired format:\n",
      "Sentiment: <identified_sentiment>\n",
      "Do not print \"Sentiment:\" again, just the sentiment.\n",
      "\n",
      "Movie review:\n",
      "```\n",
      "<movie review>\n",
      "```\n",
      "Sentiment: ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(label):\n",
    "    return \"Positive\" if label else \"Negative\"\n",
    "    # return \"positive\" if label else \"negative\"\n",
    "\n",
    "# Dataset template\n",
    "def create_get_prompt_fn(tokenizer_name):\n",
    "    def get_prompt(text, answer_prefix=\"Sentiment:\"):\n",
    "        conv = get_conversation_template(tokenizer_name)\n",
    "\n",
    "        message = (\n",
    "            \"Classify the movie review as either positive or negative.\\n\\n\"\n",
    "            \"Desired format:\\n\"\n",
    "            \"Sentiment: <identified_sentiment>\\n\"\n",
    "            f\"Do not print \\\"{answer_prefix}\\\" again, just the sentiment.\\n\\n\"\n",
    "            f\"Movie review:\\n```\\n{text}\\n```\\n\"\n",
    "            f\"{answer_prefix}\"\n",
    "        )\n",
    "\n",
    "        conv.append_message(conv.roles[0], message)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "\n",
    "        return conv.get_prompt()\n",
    "\n",
    "    return get_prompt\n",
    "\n",
    "get_prompt = create_get_prompt_fn(tokenizer_name)\n",
    "input_text = get_prompt(\"<movie review>\")\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading policy model...\n",
      "\n",
      "is_bf16_possible=False\n",
      "kwargs={'load_in_8bit': True, 'torch_dtype': None}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e181f765a7e9485f952485d9ca56f5f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded subject model with 6,738,419,713 parameters.\n",
      "Model dtype: torch.float16\n",
      "\n",
      "memory_usage=6.58 GB\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "# model = get_model(tokenizer_name, device)\n",
    "model = get_model(tokenizer_name, device, load_in_8bit=True)\n",
    "\n",
    "memory_usage = model.pretrained_model.get_memory_footprint() / (1024 ** 3)\n",
    "print(f\"{memory_usage=:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_max_len = max(\n",
    "    len(row[\"input_ids\"]) for row in processed_dataset\n",
    ")\n",
    "prompt_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /admin/home-augustas/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-707ac42cae4740a3_*_of_00012.arrow\n",
      "Loading cached processed dataset at /admin/home-augustas/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-a04ea056e5dee5a2_*_of_00012.arrow\n",
      "Loading cached processed dataset at /admin/home-augustas/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-682f7216ae3da56e.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'prompt', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 24989\n",
       "})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def apply_prompt(batch):\n",
    "    processed_batch = defaultdict(list)\n",
    "    for item in batch[\"text\"]:\n",
    "        processed_batch[\"prompt\"].append(get_prompt(item))\n",
    "    \n",
    "    return processed_batch\n",
    "\n",
    "def filter_too_long(batch):\n",
    "    # Substract a bit more to allow for generations to be processed\n",
    "    return [len(x) < tokenizer.model_max_length - 8 for x in batch[\"input_ids\"]]\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "    lambda batch: apply_prompt(batch), batched=True, num_proc=12\n",
    ")\n",
    "processed_dataset = processed_dataset.map(\n",
    "    lambda batch: tokenizer(batch[\"prompt\"]), batched=True, num_proc=12\n",
    ")\n",
    "processed_dataset = processed_dataset.filter(\n",
    "    filter_too_long,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Remove some columns\n",
    "processed_dataset = processed_dataset.remove_columns([\"token_type_ids\"])\n",
    "\n",
    "processed_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], output_all_columns=True)\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer, PPOConfig\n",
    "\n",
    "config = PPOConfig()\n",
    "\n",
    "optimizer = None\n",
    "\n",
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config,\n",
    "    model,\n",
    "    ref_model=None,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    data_collator=collator,\n",
    "    optimizer=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader length: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2654e026c04bbd916304a68e4aa48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    processed_dataset.select(range(32)),\n",
    "    batch_size=4, collate_fn=collator,\n",
    "    num_workers=12, shuffle=False,\n",
    ")\n",
    "print(f\"Dataloader length: {len(dataloader)}\")\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": False,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": 100_000, # why is this value like this?\n",
    "    \"pad_to_multiple_of\": 8, # TODO: double-check, but this seems to work and to be faster\n",
    "    \"max_new_tokens\": 4,\n",
    "}\n",
    "\n",
    "gold_outputs = []\n",
    "outputs = []\n",
    "for batch in tqdm(dataloader, total=len(dataloader), leave=False):\n",
    "    batch_gold_outputs = [get_sentiment(label) for label in batch[\"label\"]]\n",
    "    gold_outputs.extend(batch_gold_outputs)\n",
    "\n",
    "    question_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    response_tensors = ppo_trainer.generate(\n",
    "        question_tensors,\n",
    "        return_prompt=False,\n",
    "        # length_sampler=output_length_sampler, # TODO: can be none\n",
    "        batch_size=4, # TODO: generations are made in batches\n",
    "        **generation_kwargs,\n",
    "    )\n",
    "    batch[\"response\"] = tokenizer.batch_decode(\n",
    "        response_tensors, skip_special_tokens=True, spaces_between_special_tokens=False\n",
    "    )\n",
    "\n",
    "    outputs.extend(batch[\"response\"])\n",
    "\n",
    "len(outputs), len(gold_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "golden_output='Negative'\n",
      "output='Positive'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Positive'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Negative'\n",
      "output='Negative'\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for output, golden_output in zip(outputs, gold_outputs):\n",
    "    print(f\"{golden_output=}\\n{output=}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc35c07343ce44c5836c49ef090b5905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label='Negative'\n",
      "outputs='Positive'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "label='Negative'\n",
      "outputs='Negative'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "label='Negative'\n",
      "outputs='Negative'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "label='Negative'\n",
      "outputs='Negative'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "label='Negative'\n",
      "outputs='Negative'\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TODO: do I need to set the pad_token?\n",
    "generation_kwargs = {\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": False,\n",
    "    # \"pad_to_multiple_of\": 8,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": 100_000, # why is this value like this?\n",
    "    \"max_new_tokens\": 4,\n",
    "}\n",
    "\n",
    "for i, example in tqdm(enumerate(processed_dataset), total=len(dataset), leave=False):\n",
    "    label = get_sentiment(example[\"label\"])\n",
    "    print(f\"{label=}\")\n",
    "\n",
    "    inputs = {\n",
    "        \"input_ids\": example[\"input_ids\"].unsqueeze(0).to(device),\n",
    "        \"attention_mask\": example[\"attention_mask\"].unsqueeze(0).to(device),\n",
    "    }\n",
    "    # print(inputs)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        **generation_kwargs,\n",
    "    )\n",
    "    output_ids = output_ids[0][len(inputs[\"input_ids\"][0]):]\n",
    "    outputs = tokenizer.decode(\n",
    "        output_ids, skip_special_tokens=True, spaces_between_special_tokens=False\n",
    "    )\n",
    "    print(f\"{outputs=}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    if i >= 4: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Classify the movie review as either positive or negative.\n",
      "\n",
      "Desired format:\n",
      "Sentiment: <identified_sentiment>\n",
      "Do not print \"Sentiment:\" again, just the sentiment.\n",
      "\n",
      "Movie review:\n",
      "```\n",
      "Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..<br /><br />\"Is that all there is??\" ...I was just an early teen when this smoked fish hit the U.S. I was too young to get in the theater (although I did manage to sneak into \"Goodbye Columbus\"). Then a screening at a local film museum beckoned - Finally I could see this film, except now I was as old as my parents were when they schlepped to see it!!<br /><br />The ONLY reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its U.S. release. MILLIONS of people flocked to this stinker, thinking they were going to see a sex film...Instead, they got lots of closeups of gnarly, repulsive Swedes, on-street interviews in bland shopping malls, asinie political pretension...and feeble who-cares simulated sex scenes with saggy, pale actors.<br /><br />Cultural icon, holy grail, historic artifact..whatever this thing was, shred it, burn it, then stuff the ashes in a lead box!<br /><br />Elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings..But if it weren't for the censorship scandal, it would have been ignored, then forgotten.<br /><br />Instead, the \"I Am Blank, Blank\" rhythymed title was repeated endlessly for years as a titilation for porno films (I am Curious, Lavender - for gay films, I Am Curious, Black - for blaxploitation films, etc..) and every ten years or so the thing rises from the dead, to be viewed by a new generation of suckers who want to see that \"naughty sex film\" that \"revolutionized the film industry\"...<br /><br />Yeesh, avoid like the plague..Or if you MUST see it - rent the video and fast forward to the \"dirty\" parts, just to get it over with.<br /><br />\n",
      "```\n",
      "Sentiment:\n",
      "ASSISTANT: Negative\n"
     ]
    }
   ],
   "source": [
    "print(f\"{conv.roles[0]}: {input_text}\")\n",
    "print(f\"{conv.roles[1]}: {outputs}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
