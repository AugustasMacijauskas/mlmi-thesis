{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "import datasets\n",
    "# Don't show progress datasets bars\n",
    "datasets.disable_progress_bar()\n",
    "from datasets import load_dataset\n",
    "\n",
    "from fastchat.model import get_conversation_template\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd().parent.resolve()))\n",
    "from model import get_model\n",
    "from dataset import collator\n",
    "from utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    f\"cuda:{torch.cuda.current_device()}\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"lmsys/vicuna-7b-v1.3\"\n",
    "\n",
    "dataset_path = \"glue\"\n",
    "dataset_name = \"qnli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer lmsys/vicuna-7b-v1.3...\n",
      "Loaded tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = get_tokenizer(tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/admin/home-augustas/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'sentence', 'label', 'idx'],\n",
      "    num_rows: 104743\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Dataset for PPO training\n",
    "dataset = load_dataset(dataset_path, dataset_name, split=\"train\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Consider the sentence below in triple backticks and corresponding question. Does the sentence contain enough information to answer the question? Your answer should be either yes or no.\n",
      "\n",
      "Desired format:\n",
      "Answer: <your_answer>\n",
      "Do not print \"Answer:\" again, just what you think the answer is.\n",
      "\n",
      "Sentence:\n",
      "```\n",
      "He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer.\n",
      "```\n",
      "Question: What two things does Popper argue Tarski's theory involves in an evaluation of truth??\n",
      "Answer: ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "# doc_to_text: \"{{passage}}\\nQuestion: {{question}}?\\nAnswer:\"\n",
    "\n",
    "def format_label(label):\n",
    "    return \"No\" if label else \"Yes\"\n",
    "\n",
    "# Dataset template\n",
    "def create_get_prompt_fn(tokenizer_name):\n",
    "    def get_prompt(example, answer_prefix=\"Answer:\"):\n",
    "        conv = get_conversation_template(tokenizer_name)\n",
    "\n",
    "        message = (\n",
    "            \"Consider the sentence below in triple backticks \"\n",
    "            \"and corresponding question. Does the sentence contain enough information \"\n",
    "            \"to answer the question? Your answer should be either yes or no.\\n\\n\"\n",
    "            \"Desired format:\\n\"\n",
    "            \"Answer: <your_answer>\\n\"\n",
    "            f\"Do not print \\\"{answer_prefix}\\\" again, just what you think the answer is.\\n\\n\"\n",
    "            f\"Sentence:\\n```\\n{example['sentence']}\\n```\\n\"\n",
    "            f\"Question: {example['question']}?\\n\"\n",
    "            f\"{answer_prefix}\"\n",
    "        )\n",
    "\n",
    "        conv.append_message(conv.roles[0], message)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "\n",
    "        return conv.get_prompt()\n",
    "\n",
    "    return get_prompt\n",
    "\n",
    "get_prompt = create_get_prompt_fn(tokenizer_name)\n",
    "input_text = get_prompt(dataset[2])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading policy model...\n",
      "\n",
      "is_bf16_possible=False\n",
      "kwargs={'load_in_8bit': True, 'low_cpu_mem_usage': True, 'torch_dtype': None}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e77111305854541bcb84212417632bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded subject model with 6,738,419,713 parameters.\n",
      "Model dtype: torch.float16\n",
      "\n",
      "memory_usage=6.58 GB\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "# model = get_model(tokenizer_name, device)\n",
    "model = get_model(tokenizer_name, device, load_in_8bit=True, low_cpu_mem_usage=True)\n",
    "\n",
    "memory_usage = model.pretrained_model.get_memory_footprint() / (1024 ** 3)\n",
    "print(f\"{memory_usage=:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /admin/home-augustas/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e890cdbfe8f851c1_*_of_00012.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'sentence', 'label', 'idx', 'prompt', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 104743\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_prompt(batch):\n",
    "    processed_batch = defaultdict(list)\n",
    "    for sentence, question in zip(batch[\"sentence\"], batch[\"question\"]):\n",
    "        example = { \"sentence\": sentence, \"question\": question }\n",
    "        processed_batch[\"prompt\"].append(get_prompt(example))\n",
    "    \n",
    "    return processed_batch\n",
    "\n",
    "def filter_too_long(batch):\n",
    "    # Substract a bit more to allow for generations to be processed\n",
    "    return [len(x) < tokenizer.model_max_length - 8 for x in batch[\"input_ids\"]]\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "    lambda batch: apply_prompt(batch), batched=True, num_proc=12\n",
    ")\n",
    "processed_dataset = processed_dataset.map(\n",
    "    lambda batch: tokenizer(batch[\"prompt\"]), batched=True, num_proc=12\n",
    ")\n",
    "processed_dataset = processed_dataset.filter(filter_too_long, batched=True, num_proc=12)\n",
    "\n",
    "# Remove some columns\n",
    "processed_dataset = processed_dataset.remove_columns([\"token_type_ids\"])\n",
    "\n",
    "processed_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], output_all_columns=True)\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the processed dataset to hub\n",
    "# processed_dataset.push_to_hub(\"AugustasM/imdb_vicuna\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer, PPOConfig\n",
    "\n",
    "config = PPOConfig()\n",
    "\n",
    "optimizer = None\n",
    "\n",
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config,\n",
    "    model,\n",
    "    ref_model=None,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    data_collator=collator,\n",
    "    optimizer=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_fn(text_batch):\n",
    "    outputs = []\n",
    "\n",
    "    for text in text_batch:\n",
    "        if \"Yes\" in text:\n",
    "            outputs.append(\"Yes\")\n",
    "        elif \"No\" in text:\n",
    "            outputs.append(\"No\")\n",
    "        else:\n",
    "            outputs.append(text)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader length: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92dfeec330e543c7874ff4a9a2907dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    # processed_dataset,\n",
    "    processed_dataset.shuffle(seed=42).select(range(8)),\n",
    "    # processed_dataset.shuffle(seed=42).select(range(1024)),\n",
    "    batch_size=32, collate_fn=collator,\n",
    "    num_workers=12, shuffle=False,\n",
    ")\n",
    "print(f\"Dataloader length: {len(dataloader)}\")\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": False,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": 100_000, # why is this value like this?\n",
    "    \"pad_to_multiple_of\": 8, # TODO: double-check, but this seems to work and to be faster\n",
    "    \"max_new_tokens\": 1,\n",
    "}\n",
    "\n",
    "gold_outputs = []\n",
    "outputs = []\n",
    "for batch in tqdm(dataloader, total=len(dataloader), leave=False):\n",
    "    batch_gold_outputs = [format_label(label) for label in batch[\"label\"]]\n",
    "    gold_outputs.extend(batch_gold_outputs)\n",
    "\n",
    "    question_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    response_tensors = ppo_trainer.generate(\n",
    "        question_tensors,\n",
    "        return_prompt=False,\n",
    "        # length_sampler=output_length_sampler, # TODO: can be none\n",
    "        batch_size=8, # TODO: generations are made in batches\n",
    "        **generation_kwargs,\n",
    "    )\n",
    "    batch[\"response\"] = tokenizer.batch_decode(\n",
    "        response_tensors, skip_special_tokens=True, spaces_between_special_tokens=False\n",
    "    )\n",
    "\n",
    "    # Postprocess\n",
    "    # batch[\"response\"] = postprocess_fn(batch[\"response\"])\n",
    "\n",
    "    outputs.extend(batch[\"response\"])\n",
    "\n",
    "len(outputs), len(gold_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "golden_output='Yes'\n",
      "output='Appar'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='No'\n",
      "output='No'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='No'\n",
      "output='No'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='No'\n",
      "output='Yes'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Yes'\n",
      "output='Yes'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Yes'\n",
      "output='Yes'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Yes'\n",
      "output='Yes'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='Yes'\n",
      "output='Yes'\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for output, golden_output in zip(outputs, gold_outputs):\n",
    "    print(f\"{golden_output=}\\n{output=}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n",
      "Counter({'Yes': 5, 'No': 2, 'Appar': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(len(gold_outputs), len(outputs))\n",
    "print(Counter(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_accuracy(gold_outputs, outputs):\n",
    "    return sum([1 if gold == output else 0 for gold, output in zip(gold_outputs, outputs)]) / len(outputs)\n",
    "\n",
    "get_accuracy(gold_outputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "golden_output='Yes'\n",
      "output='Appar'\n",
      "--------------------------------------------------------------------------------\n",
      "golden_output='No'\n",
      "output='Yes'\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for output, golden_output in zip(outputs, gold_outputs):\n",
    "    if output != golden_output:\n",
    "        print(f\"{golden_output=}\\n{output=}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
