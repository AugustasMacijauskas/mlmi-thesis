{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset dbpedia_14 (/admin/home-augustas/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'title', 'content'],\n",
       "    num_rows: 70000\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset_name = \"dbpedia_14\"\n",
    "dataset = load_dataset(dataset_name, split=\"test\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /admin/home-augustas/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c/cache-fe7abc7bcca974aa.arrow\n",
      "Pushing split test to the Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0, 'title': 'TY KU', 'content': \" TY KU /taɪkuː/ is an American alcoholic beverage company that specializes in sake and other spirits. The privately-held company was founded in 2004 and is headquartered in New York City New York. While based in New York TY KU's beverages are made in Japan through a joint venture with two sake breweries. Since 2011 TY KU's growth has extended its products into all 50 states.\", 'idx': 0}\n",
      "70000\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['label', 'title', 'content', 'idx'],\n",
      "        num_rows: 70000\n",
      "    })\n",
      "})\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "dict_keys(['label', 'title', 'content'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651b528e7d4f4a448ec76a2cdcf16e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215ffe3a79d945f786d15368d72f106b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/70 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d983ba0ae44146d0a459f9a92ba5bb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a unique index to each row\n",
    "def add_index(example, idx):\n",
    "    example['idx'] = idx\n",
    "    return example\n",
    "\n",
    "processed_dataset = dataset.map(add_index, with_indices=True)\n",
    "\n",
    "print(processed_dataset[0])\n",
    "print(len(set(processed_dataset[\"idx\"])))\n",
    "\n",
    "processed_dataset = DatasetDict({ \"test\": processed_dataset })\n",
    "print(processed_dataset)\n",
    "\n",
    "print(processed_dataset[\"test\"][\"idx\"][:10])\n",
    "\n",
    "print(dataset.features.keys())\n",
    "\n",
    "# Save the dataset\n",
    "# processed_dataset.push_to_hub(f\"AugustasM/{dataset_name}_with_indices\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset hellaswag (/admin/home-augustas/.cache/huggingface/datasets/hellaswag/default/0.1.0/512a66dd8b1b1643ab4a48aa4f150d04c91680da6a4096498a5e5f799623d5ae)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['ind', 'activity_label', 'ctx_a', 'ctx_b', 'ctx', 'endings', 'source_id', 'split', 'split_type', 'label'],\n",
      "    num_rows: 10042\n",
      "})\n",
      "Counter({'2': 2584, '0': 2515, '1': 2485, '3': 2458})\n",
      "2\n",
      "Majority class baseline accuracy: 25.73\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "# dataset = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
    "dataset = load_dataset(\"truthfulqa_mc\", split=\"validation\")\n",
    "print(dataset)\n",
    "\n",
    "# Majority class baseline\n",
    "counts = Counter(dataset[\"label\"])\n",
    "print(counts)\n",
    "most_freq_label = counts.most_common(1)[0][0]\n",
    "print(most_freq_label)\n",
    "print(f\"Majority class baseline accuracy: {counts[most_freq_label] / len(dataset) * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ai2_arc (/admin/home-augustas/.cache/huggingface/datasets/ai2_arc/ARC-Challenge/1.0.0/1569c2591ea2683779581d9fb467203d9aa95543bb9b75dcfde5da92529fd7f6)\n",
      "Loading cached processed dataset at /admin/home-augustas/.cache/huggingface/datasets/ai2_arc/ARC-Challenge/1.0.0/1569c2591ea2683779581d9fb467203d9aa95543bb9b75dcfde5da92529fd7f6/cache-ed441d7b93e96de2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'question', 'choices', 'answerKey'],\n",
      "    num_rows: 1172\n",
      "})\n",
      "['A', 'B', 'C', 'D']\n",
      "C\n",
      "[2, 1, 2, 3, 3, 1, 2, 2, 1, 0]\n",
      "Counter({1: 311, 2: 310, 3: 285, 0: 266})\n",
      "1\n",
      "Majority class baseline accuracy: 26.54\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "dataset = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
    "print(dataset)\n",
    "\n",
    "print(dataset[0][\"choices\"][\"label\"])\n",
    "print(dataset[0][\"answerKey\"])\n",
    "\n",
    "dataset = dataset.map(lambda example: { \"label\": example[\"choices\"][\"label\"].index(example[\"answerKey\"]) })\n",
    "print(dataset[\"label\"][:10])\n",
    "\n",
    "# Majority class baseline\n",
    "counts = Counter(dataset[\"label\"])\n",
    "print(counts)\n",
    "most_freq_label = counts.most_common(1)[0][0]\n",
    "print(most_freq_label)\n",
    "print(f\"Majority class baseline accuracy: {counts[most_freq_label] / len(dataset) * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 2, 3, 3, 1, 2, 2, 1, 0]\n",
      "Counter({1: 311, 2: 310, 3: 285, 0: 266})\n",
      "1\n",
      "Majority class baseline accuracy: 26.54\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"label\"][:10])\n",
    "\n",
    "# Majority class baseline\n",
    "counts = Counter(dataset[\"label\"])\n",
    "print(counts)\n",
    "most_freq_label = counts.most_common(1)[0][0]\n",
    "print(most_freq_label)\n",
    "print(f\"Majority class baseline accuracy: {counts[most_freq_label] / len(dataset) * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset amazon_polarity (/admin/home-augustas/.cache/huggingface/datasets/amazon_polarity/amazon_polarity/3.0.0/a27b32b7e7b88eb274a8fa8ba0f654f1fe998a87c22547557317793b5d2772dc)\n",
      "Loading cached shuffled indices for dataset at /admin/home-augustas/.cache/huggingface/datasets/amazon_polarity/amazon_polarity/3.0.0/a27b32b7e7b88eb274a8fa8ba0f654f1fe998a87c22547557317793b5d2772dc/cache-0a5d0b47b5e8dfc6.arrow\n",
      "Loading cached processed dataset at /admin/home-augustas/.cache/huggingface/datasets/amazon_polarity/amazon_polarity/3.0.0/a27b32b7e7b88eb274a8fa8ba0f654f1fe998a87c22547557317793b5d2772dc/cache-f82de97f5c546404_*_of_00012.arrow\n",
      "Loading cached processed dataset at /admin/home-augustas/.cache/huggingface/datasets/amazon_polarity/amazon_polarity/3.0.0/a27b32b7e7b88eb274a8fa8ba0f654f1fe998a87c22547557317793b5d2772dc/cache-3755e47a04d615ff_*_of_00012.arrow\n",
      "Loading cached shuffled indices for dataset at /admin/home-augustas/.cache/huggingface/datasets/amazon_polarity/amazon_polarity/3.0.0/a27b32b7e7b88eb274a8fa8ba0f654f1fe998a87c22547557317793b5d2772dc/cache-836e58b1f72f6342.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 50000, 0: 50000})\n",
      "1\n",
      "Majority class baseline accuracy: 50.00\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['label', 'title', 'content'],\n",
      "        num_rows: 100000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split test to the Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2bf15d1eb564a3996789053d59ccd3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac22a05089948c29df822ab3127dd0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/516 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "dataset = load_dataset(\"amazon_polarity\", split=\"test\")\n",
    "\n",
    "# Get a stratified split\n",
    "processed_dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Take the first n samples\n",
    "pos_samples = processed_dataset.filter(lambda x: x[\"label\"] == 1, num_proc=12).select(range(50000))\n",
    "neg_samples = processed_dataset.filter(lambda x: x[\"label\"] == 0, num_proc=12).select(range(50000))\n",
    "\n",
    "# Join train and test splits into one\n",
    "processed_dataset = concatenate_datasets([pos_samples, neg_samples]).shuffle(seed=42)\n",
    "\n",
    "# Majority class baseline\n",
    "counts = Counter(processed_dataset[\"label\"])\n",
    "print(counts)\n",
    "most_freq_label = counts.most_common(1)[0][0]\n",
    "print(most_freq_label)\n",
    "print(f\"Majority class baseline accuracy: {counts[most_freq_label] / len(processed_dataset) * 100:.2f}\")\n",
    "\n",
    "processed_dataset = DatasetDict({ \"test\": processed_dataset })\n",
    "print(processed_dataset)\n",
    "\n",
    "print(processed_dataset[\"test\"][\"label\"][:10])\n",
    "\n",
    "# Save the dataset\n",
    "# processed_dataset.push_to_hub(f\"AugustasM/amazon_polarity_subsample\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /admin/home-augustas/.cache/huggingface/datasets/AugustasM___parquet/AugustasM--amazon_polarity_subsample-be3d73de71c20033/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fef630cbce4a6fa3e899568b0dfdfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d94eb4340204d4aa1e49b8b0b364658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/29.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23c9a5d106c4b42ae7f9fd366e0d3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86d6ec51e0e48afbc8062fef9558696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /admin/home-augustas/.cache/huggingface/datasets/AugustasM___parquet/AugustasM--amazon_polarity_subsample-be3d73de71c20033/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad42c7e194594cacb0504342a0a70ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['label', 'title', 'content'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"AugustasM/amazon_polarity_subsample\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"microsoft/deberta-v3-large\"\n",
    "model_name = \"microsoft/deberta-v2-xxlarge\"\n",
    "model_name = \"EleutherAI/gpt-j-6b\"\n",
    "model_name = \"databricks/dolly-v2-3b\"\n",
    "model_name = \"gpt2\"\n",
    "model_name = \"gpt2-xl\"\n",
    "# model_name = \"allenai/unifiedqa-v2-t5-11b-1363200\"\n",
    "# model_name = \"allenai/unifiedqa-v2-t5-3b-1363200\"\n",
    "# model_name = \"allenai/unifiedqa-v2-t5-large-1363200\"\n",
    "# model_name = \"allenai/unifiedqa-v2-t5-base-1363200\"\n",
    "# model_name = \"allenai/unifiedqa-v2-t5-small-1363200\"\n",
    "\n",
    "model_cfg = AutoConfig.from_pretrained(model_name)\n",
    "model_cfg.is_encoder_decoder, model_cfg.is_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg.torch_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GPT2LMHeadModel']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cfg.architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cfg.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2-xl', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params: 1,557,611,200\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(model_name)\n",
    "print(f\"# params: {sum(p.numel() for p in model.parameters()):,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8750477880f64d63a66b9b78002f2493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d7594c8b8440bea68030b1d4d0bef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params: 737,668,096\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "print(f\"# params: {sum(p.numel() for p in model.parameters()):,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params: 2,775,086,080\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoXForCausalLM\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(model_name)\n",
    "print(f\"# params: {sum(p.numel() for p in model.parameters()):,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50280, 2560)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gpt_neox.embed_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.to(\"cuda:0\")\n",
    "\n",
    "# More than 2048 does not work\n",
    "x = torch.randint(0, tokenizer.vocab_size, (2, 2048)).to(\"cuda:0\")\n",
    "x\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(x)\n",
    "\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2048, 50280])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
