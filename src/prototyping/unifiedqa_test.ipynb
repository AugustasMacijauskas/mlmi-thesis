{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_2P-ySaIzlxi"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/fsx/home-augustas/elk\n",
            "['/fsx/home-augustas/elk/elk/promptsource', '/fsx/home-augustas/elk/elk/training', '/fsx/home-augustas/elk']\n"
          ]
        }
      ],
      "source": [
        "ELK_PATH = Path(\"../../../elk/\")\n",
        "print(ELK_PATH.resolve())\n",
        "\n",
        "modules = [\n",
        "    ELK_PATH,\n",
        "    ELK_PATH / \"elk\" / \"training\",\n",
        "    ELK_PATH / \"elk\" / \"promptsource\",\n",
        "]\n",
        "\n",
        "for module in modules:\n",
        "    if not str(module) in sys.path:\n",
        "        sys.path.insert(0, str(module.resolve()))\n",
        "\n",
        "print(sys.path[:3])\n",
        "\n",
        "from reporter import Reporter\n",
        "from templates import DatasetTemplates"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmxqZN-j0Hcg",
        "outputId": "575c90f7-1754-4b86-bf92-cd5e3d804859"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset imdb (/admin/home-augustas/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 64\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"imdb\", split=\"train[:64]\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Consider the text below': <templates.Template object at 0x7f70f406e860>, 'Is the given text truthful': <templates.Template object at 0x7f70f406e890>, 'Text first': <templates.Template object at 0x7f70f406e8c0>, 'No question no choices': <templates.Template object at 0x7f70f406d750>, 'No question with choices': <templates.Template object at 0x7f70f406e800>}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<templates.Template at 0x7f70f406e860>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_template_path = \"AugustasM/burns-datasets-VINC/all\"\n",
        "\n",
        "dataset_templates = DatasetTemplates(dataset_template_path)\n",
        "dataset_templates.templates = {\n",
        "    x.name: x for x in dataset_templates.templates.values() if x.get_answer_choices_list(dataset[0]) is not None\n",
        "}\n",
        "print(dataset_templates.templates)\n",
        "\n",
        "template = list(dataset_templates.templates.values())[0]\n",
        "template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'no'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q, a = template.apply(dataset[0])\n",
        "a"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqhjctAQzy5S",
        "outputId": "da429f7f-8b54-432a-d7d0-786d9cfbab0e"
      },
      "outputs": [],
      "source": [
        "# model_name = \"allenai/unifiedqa-v2-t5-11b-1363200\"\n",
        "model_name = \"allenai/unifiedqa-v2-t5-3b-1363200\"\n",
        "# model_name = \"allenai/unifiedqa-t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name, truncation_side=\"left\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp__H8KQ0uoT",
        "outputId": "7582fadc-5dfa-4da3-e5ac-a99cc8e2951b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 459])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_batch = tokenizer(q, add_special_tokens=True, return_tensors=\"pt\", text_target=a.strip()).to(device)\n",
        "text_batch[\"input_ids\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24DYLjBpSe_b",
        "outputId": "de4243fe-93b4-4ae7-c2eb-33fea72d475c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'labels'])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_batch.keys()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wWEcJ5wW8CLJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "# params: 2851598336\n",
            "CPU times: user 29.5 s, sys: 21.6 s, total: 51.1 s\n",
            "Wall time: 55.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "print(model.config.is_encoder_decoder)\n",
        "print(hasattr(model, \"get_encoder\") and callable(model.get_encoder))\n",
        "print(f\"# params: {sum(p.numel() for p in model.parameters())}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "5L6ILFzIRBM2",
        "outputId": "c2767664-3fce-4918-921e-b3e550ec7be5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "odict_keys(['loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'encoder_last_hidden_state', 'encoder_hidden_states'])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(**text_batch, output_hidden_states=True)\n",
        "\n",
        "outputs.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25, 25)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(outputs[\"encoder_hidden_states\"]), len(outputs[\"decoder_hidden_states\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024]),\n",
              " torch.Size([1, 2, 1024])]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hiddens = outputs[\"decoder_hidden_states\"]\n",
        "[x.shape for x in hiddens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024]),\n",
              " torch.Size([1, 1024])]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[h[:, -1, :].shape for h in hiddens]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoder only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "# params: 35330816\n"
          ]
        }
      ],
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "print(model.config.is_encoder_decoder)\n",
        "if hasattr(model, \"get_encoder\") and callable(model.get_encoder):\n",
        "    model = model.get_encoder()\n",
        "\n",
        "print(f\"# params: {sum(p.numel() for p in model.parameters())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "model_cfg = AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "# Ordered by preference\n",
        "_DECODER_ONLY_SUFFIXES = [\n",
        "    \"CausalLM\",\n",
        "    \"LMHeadModel\",\n",
        "]\n",
        "# Includes encoder-decoder models\n",
        "_AUTOREGRESSIVE_SUFFIXES = [\"ConditionalGeneration\"] + _DECODER_ONLY_SUFFIXES\n",
        "\n",
        "def is_autoregressive(model_cfg, include_enc_dec: bool) -> bool:\n",
        "    \"\"\"Check if a model config is autoregressive.\"\"\"\n",
        "    archs = model_cfg.architectures\n",
        "    if not isinstance(archs, list):\n",
        "        return False\n",
        "\n",
        "    suffixes = _AUTOREGRESSIVE_SUFFIXES if include_enc_dec else _DECODER_ONLY_SUFFIXES\n",
        "    return any(arch_str.endswith(suffix) for arch_str in archs for suffix in suffixes)\n",
        "\n",
        "has_lm_preds = is_autoregressive(model_cfg, include_enc_dec=False)\n",
        "has_lm_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 459])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 460])"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_ids = text_batch.input_ids\n",
        "print(question_ids.shape)\n",
        "\n",
        "answer_tokenized = tokenizer(\n",
        "    a.strip(),\n",
        "    # Don't include [CLS] and [SEP] in the answer\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\",\n",
        ").to(device)\n",
        "\n",
        "answer_ids = answer_tokenized.input_ids\n",
        "ids = torch.cat([question_ids, answer_ids], -1)\n",
        "ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "odict_keys(['last_hidden_state', 'hidden_states'])"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(input_ids=ids.long(), output_hidden_states=True)\n",
        "\n",
        "outputs.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[torch.Size([1, 460, 512]),\n",
              " torch.Size([1, 460, 512]),\n",
              " torch.Size([1, 460, 512]),\n",
              " torch.Size([1, 460, 512]),\n",
              " torch.Size([1, 460, 512]),\n",
              " torch.Size([1, 460, 512]),\n",
              " torch.Size([1, 460, 512])]"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hiddens = outputs[\"hidden_states\"]\n",
        "[x.shape for x in hiddens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[torch.Size([1, 512]),\n",
              " torch.Size([1, 512]),\n",
              " torch.Size([1, 512]),\n",
              " torch.Size([1, 512]),\n",
              " torch.Size([1, 512]),\n",
              " torch.Size([1, 512]),\n",
              " torch.Size([1, 512])]"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[h[:, -1, :].shape for h in hiddens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
