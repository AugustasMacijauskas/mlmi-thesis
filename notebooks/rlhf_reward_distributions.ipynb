{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/am3052/.conda/envs/elk/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        self.model(input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"EleutherAI/gpt-j-6b\", cache_dir=\"../../.hf_cache/hub\"\n",
    "        )\n",
    "\n",
    "        self.transformer = model.transformer\n",
    "        self.v_head = nn.Linear(model.config.n_embd, 1, bias=False, dtype=torch.float16)\n",
    "        \n",
    "        self.PAD_ID = model.config.eos_token_id\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        hidden_states = self.transformer(\n",
    "            input_ids, attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "\n",
    "        rewards = self.v_head(hidden_states).squeeze(-1)\n",
    "        \n",
    "        ends = (input_ids == self.PAD_ID).int().argmax(dim=1, keepdim=True)\n",
    "        rewards = torch.gather(rewards, 1, ends)\n",
    "        \n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model (might take a few minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = (\n",
    "    \"../../.hf_cache/hub/models--Dahoas--gptj-rm-static/\"\n",
    "    \"snapshots/dc9bb2f15f4cddace8a812174c3e7afda2308258/hf_ckpt.pt\"\n",
    ")\n",
    "\n",
    "reward_model = RewardModel()\n",
    "reward_model.load_state_dict(torch.load(WEIGHTS_PATH), strict=True)\n",
    "reward_model.to(DEVICE)\n",
    "reward_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = reward_model(\n",
    "        input_ids=tokenized_text[\"chosen\"].to(DEVICE),\n",
    "        attention_mask=tokenized_text[\"chosen_attention_mask\"].to(DEVICE)\n",
    "    )\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Dahoas/rm-static\", split=\"test\", cache_dir=\"../../.hf_cache/datasets\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1024, 1024, 1024, 1024],\n",
       " ['chosen', 'rejected', 'chosen_attention_mask', 'rejected_attention_mask'],\n",
       " tensor([[  198,   198, 20490,    25,   314,   716,  2111,   284,  3551,   257]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_row(row, tokenizer, max_length=1024):\n",
    "    prompt = row[\"prompt\"]\n",
    "    chosen_response, rejected_response = row[\"chosen\"], row[\"rejected\"]\n",
    "\n",
    "    # Note that we do not have to truncate in this specific case since\n",
    "    # all input sequences will be shorter than max_length\n",
    "    chosen_tokenized = tokenizer(\n",
    "        prompt + chosen_response + EOS_TOKEN,\n",
    "        padding=\"max_length\", max_length=max_length, return_tensors=\"pt\",\n",
    "    )\n",
    "    rejected_tokenized = tokenizer(\n",
    "        prompt + rejected_response + EOS_TOKEN,\n",
    "        padding=\"max_length\", max_length=max_length, return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"chosen\": chosen_tokenized[\"input_ids\"],\n",
    "        \"rejected\": rejected_tokenized[\"input_ids\"],\n",
    "        \"chosen_attention_mask\": chosen_tokenized[\"attention_mask\"],\n",
    "        \"rejected_attention_mask\": rejected_tokenized[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "tokenized_text = tokenize_row(dataset[0], tokenizer)\n",
    "list(map(lambda x: x.shape[1], tokenized_text.values())), list(tokenized_text.keys()), tokenized_text[\"chosen\"][:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /rds/user/am3052/hpc-work/.hf_cache/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-5e48db60fca6235f.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected'],\n",
       "    num_rows: 5103\n",
       "})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(tokenize_row, fn_kwargs={ \"tokenizer\": tokenizer })\n",
    "dataset = dataset.remove_columns([\"prompt\", \"response\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do inferece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
